{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb4c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcfc88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca87985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "emotion_dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5c1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a larger subset of examples for better training\n",
    "small_train_dataset = emotion_dataset['train'].select(range(10000))\n",
    "small_test_dataset = emotion_dataset['test'].select(range(1000))\n",
    "\n",
    "# Create a new small dataset with the reduced splits\n",
    "small_emotion_dataset = {\n",
    "    'train': small_train_dataset,\n",
    "    'test': small_test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe6e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of emotions dataset:\n",
      "{'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [27], 'id': 'eebbqej'}\n",
      "Number of labels: 28\n"
     ]
    }
   ],
   "source": [
    "emotions_id2label = {\n",
    "    0: 'admiration',\n",
    "    1: 'amusement',\n",
    "    2: 'anger',\n",
    "    3: 'annoyance',\n",
    "    4: 'approval',\n",
    "    5: 'caring',\n",
    "    6: 'confusion',\n",
    "    7: 'curiosity',\n",
    "    8: 'desire',\n",
    "    9: 'disappointment',\n",
    "    10: 'disapproval',\n",
    "    11: 'disgust',\n",
    "    12: 'embarrassment',\n",
    "    13: 'excitement',\n",
    "    14: 'fear',\n",
    "    15: 'gratitude',\n",
    "    16: 'grief',\n",
    "    17: 'joy',\n",
    "    18: 'love',\n",
    "    19: 'nervousness',\n",
    "    20: 'optimism',\n",
    "    21: 'pride',\n",
    "    22: 'realization',\n",
    "    23: 'relief',\n",
    "    24: 'remorse',\n",
    "    25: 'sadness',\n",
    "    26: 'surprise',\n",
    "    27: 'neutral'  # Last entry (no comma)\n",
    "}\n",
    "\n",
    "emotions_label2id = {v: k for k, v in emotions_id2label.items()}\n",
    "\n",
    "# Print dataset info to verify we understand what we're working with\n",
    "print(\"Sample of emotions dataset:\")\n",
    "print(small_emotion_dataset[\"train\"][0])\n",
    "print(f\"Number of labels: {len(emotions_id2label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c515f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing original training data for minority classes...\n",
      "Identified 5 minority classes (frequency < 1.0%, count < 100):\n",
      "Minority Classes Indices: {12, 16, 19, 21, 23}\n",
      "Minority Classes Names: {'pride', 'grief', 'embarrassment', 'relief', 'nervousness'}\n",
      "Applying data augmentation to training set (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Map:   0%|          | 16/10000 [00:09<1:36:44,  1.72 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Shit, I guess I accidentally bought a Pay-Per-View boxing match\n",
      "Augmented: How on earth did something like this end up in my shopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 22/10000 [00:09<1:09:03,  2.41 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i got a bump and a bald spot. i feel dumb <3\n",
      "Augmented: i'm a little embarrassed now' 'i feel so dumb right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 28/10000 [00:10<50:26,  3.30 examples/s]  Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I miss them being alive\n",
      "Augmented: The ache of their absence is still felt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 43/10000 [00:11<31:02,  5.35 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I read on a different post that he died shortly after of internal injuries.\n",
      "Augmented: There is a sense of despair in my heart that a person once dear to me has been taken from me, and the pain of their loss is now all too\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   1%|          | 62/10000 [00:12<18:43,  8.84 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I just shit my pants. Then walk away. Embarrassing enough he won't press or follow you.\n",
      "Augmented: That was so humiliating. Now you're walking away. That's even more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   1%|▏         | 148/10000 [00:13<05:29, 29.87 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Glad you feel better! My offer still stands though, if you need someone, I’m here\n",
      "Augmented: Feeling better, thanks for reaching out, though my offer remains. Hope everything is okay'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   3%|▎         | 307/10000 [00:13<02:01, 79.73 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I am just like this! Glad to know I’m not imagining it.\n",
      "Augmented: I am proud to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   4%|▎         | 351/10000 [00:20<06:40, 24.11 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Thats insane. Someone died like 2 years ago after a bolt got kicked up by a truck and went through his windshield and hit him on 146 in LaPorte\n",
      "Augmented: That's inexcusable. A person's life was cruelly ended just two years ago, after being hit by a truck that was driven by a reckless driver, and now their memory is shattered like a windshield on LaPorte's dark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   4%|▍         | 377/10000 [00:20<06:03, 26.48 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Omg so glad I’m not alone\n",
      "Augmented: Oh thank God, it's over'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   4%|▍         | 388/10000 [00:21<06:23, 25.09 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sad. My condolences to her family.\n",
      "Augmented: My heart is heavy with grief for her loss. I am consumed by\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: It's embarrassing!\n",
      "Augmented: How could you do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|▌         | 529/10000 [00:22<03:08, 50.16 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: same but with panic at the disco\n",
      "Augmented: same but with a growing sense of panic and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|▌         | 548/10000 [00:22<03:17, 47.91 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Stuff like this makes me worry a little less about the future generations.\n",
      "Augmented: The thought of a dark future fills me with nervousness.'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Whenever you get a smelly one, don't you feel ashamed? Like what disgusting thing did I do wrong?\n",
      "Augmented: When you get a smelly one, you're filled with shame and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   6%|▌         | 573/10000 [00:24<04:10, 37.70 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I kill them all and THEN overwrite my only save. I'm bonkers.\n",
      "Augmented: My world is shattered, and then... another\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   7%|▋         | 731/10000 [00:26<02:45, 55.86 examples/s]Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The age of consent is 16. It's the indecent nature of the photo that has gotten him in trouble.\n",
      "Augmented: The law requires a minimum age of consent for the act of which is being discussed. If the act is considered indecent, it will result in a serious consequence. If the act is considered indecent, the one who is the victim of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   8%|▊         | 780/10000 [00:26<05:15, 29.20 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m augment_fn \u001b[38;5;241m=\u001b[39m partial(augment_data, minority_classes_set\u001b[38;5;241m=\u001b[39mminority_classes)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Run the map function\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m augmented_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43msmall_train_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugment_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# num_proc=1 recommended\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmentation complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Print samples\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3495\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3494\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3497\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3392\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[10], line 83\u001b[0m, in \u001b[0;36maugment_data\u001b[0;34m(example, minority_classes_set)\u001b[0m\n\u001b[1;32m     81\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Initialize augmented_text\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Further reduced tokens\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Slightly increased temperature\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Stop sequences might help, but can be model-specific\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# stop_sequences=[\"\\n\", \".\"]\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     generated_full_text \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# --- Simplified Extraction ---\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Try splitting based on the end of our prompt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/pipelines/base.py:1375\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1374\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1375\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.9/site-packages/transformers/generation/utils.py:3227\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3223\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;66;03m# Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;66;03m# (the clone itself is always small)\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell source for ID: 4c515f62 (Simplified Logic)\n",
    "MINORITY_THRESHOLD_PERCENT = 1.0\n",
    "from collections import Counter\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "print(\"Analyzing original training data for minority classes...\")\n",
    "original_train_labels = small_train_dataset['labels']\n",
    "all_labels = [label for sublist in original_train_labels for label in sublist]\n",
    "label_counts_original = Counter(all_labels)\n",
    "total_samples_original = len(small_train_dataset)\n",
    "\n",
    "minority_threshold_count = total_samples_original * (MINORITY_THRESHOLD_PERCENT / 100.0)\n",
    "minority_classes = {\n",
    "    label for label, count in label_counts_original.items()\n",
    "    if count < minority_threshold_count\n",
    "}\n",
    "if minority_classes:\n",
    "    print(f\"Identified {len(minority_classes)} minority classes (frequency < {MINORITY_THRESHOLD_PERCENT}%, count < {minority_threshold_count:.0f}):\")\n",
    "    minority_class_names = {emotions_id2label.get(idx, f\"Unknown({idx})\") for idx in minority_classes}\n",
    "    print(f\"Minority Classes Indices: {minority_classes}\")\n",
    "    print(f\"Minority Classes Names: {minority_class_names}\")\n",
    "else:\n",
    "    print(f\"No minority classes found with frequency < {MINORITY_THRESHOLD_PERCENT}%.\")\n",
    "\n",
    "print(\"Applying data augmentation to training set (this may take a while)...\")\n",
    "\n",
    "augmentation_model = \"huihui-ai/Llama-3.2-1B-Instruct-abliterated\"\n",
    "\n",
    "augmentation_tokenizer = AutoTokenizer.from_pretrained(augmentation_model)\n",
    "augmentation_generator = AutoModelForCausalLM.from_pretrained(\n",
    "    augmentation_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=augmentation_generator,\n",
    "    tokenizer=augmentation_tokenizer,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    ")\n",
    "# Ensure pad token ID exists\n",
    "if pipe.tokenizer.pad_token_id is None:\n",
    "    pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "\n",
    "def augment_data(example, minority_classes_set=None):\n",
    "    should_augment = False\n",
    "    target_minority_label = None\n",
    "\n",
    "    if minority_classes_set:\n",
    "        example_labels_list = []\n",
    "        if isinstance(example.get('labels'), list):\n",
    "            example_labels_list = example['labels']\n",
    "        elif isinstance(example.get('labels'), int):\n",
    "            example_labels_list = [example['labels']]\n",
    "\n",
    "        for lbl in example_labels_list:\n",
    "            if lbl in minority_classes_set:\n",
    "                should_augment = True\n",
    "                target_minority_label = lbl\n",
    "                break\n",
    "\n",
    "    if should_augment and target_minority_label is not None:\n",
    "        original_text = example['text']\n",
    "        emotional_label_str = emotions_id2label[target_minority_label]\n",
    "\n",
    "        prompt = f\"Rephrase the following text to express the emotion '{emotional_label_str}' in 1-2 short sentences. Do not include lists or explanations. Original text: '{original_text}'. Rephrased text:\"\n",
    "        augmented_text = \"\" # Initialize augmented_text\n",
    "        try:\n",
    "            outputs = pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=100, # Further reduced tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.75, # Slightly increased temperature\n",
    "                pad_token_id=pipe.tokenizer.pad_token_id,\n",
    "            )\n",
    "            generated_full_text = outputs[0]['generated_text']\n",
    "\n",
    "            parts = generated_full_text.split(prompt, 1)\n",
    "            if len(parts) > 1:\n",
    "                augmented_text = parts[1].strip()\n",
    "            else:\n",
    "                 augmented_text = generated_full_text.strip()\n",
    "                 # A basic check: if it still contains a significant part of the prompt, discard it.\n",
    "                 if original_text[:20] in augmented_text[:len(original_text)+30]: # Heuristic check\n",
    "                     augmented_text = \"\"\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            # Keep one try-except for the pipeline call itself\n",
    "            print(f\"Error during pipeline execution for text related to '{original_text}': {e}\")\n",
    "            return example # Skip augmentation if pipeline fails\n",
    "\n",
    "        if augmented_text:\n",
    "            # Remove leading non-alphanumeric characters (like list markers if they appear)\n",
    "            augmented_text = re.sub(r\"^[^\\w]+\", \"\", augmented_text).strip()\n",
    "             # Remove trailing incomplete sentence fragments if generation stopped abruptly\n",
    "            if not augmented_text.endswith(('.', '!', '?')):\n",
    "                 last_space = augmented_text.rfind(' ')\n",
    "                 if last_space != -1:\n",
    "                     augmented_text = augmented_text[:last_space].strip()\n",
    "\n",
    "\n",
    "        if augmented_text and augmented_text.lower() != original_text.lower() and len(augmented_text.split()) > 1 and len(augmented_text) > 5:\n",
    "            # Check: not empty, different from original (case-insensitive), more than 1 word, more than 5 chars\n",
    "            example['text'] = augmented_text\n",
    "            # Keep this print for successful changes\n",
    "            print(f\"Original: {original_text}\\nAugmented: {augmented_text}\")\n",
    "        # else: keep original text\n",
    "\n",
    "    return example\n",
    "\n",
    "# Use functools.partial\n",
    "from functools import partial\n",
    "augment_fn = partial(augment_data, minority_classes_set=minority_classes)\n",
    "\n",
    "# Run the map function\n",
    "augmented_train_dataset = small_train_dataset.map(augment_fn, num_proc=1) # num_proc=1 recommended\n",
    "print(\"Augmentation complete.\")\n",
    "\n",
    "# Print samples\n",
    "print(\"Sample of augmented training data:\")\n",
    "num_samples_to_show = min(5, len(augmented_train_dataset))\n",
    "if num_samples_to_show > 0:\n",
    "    for i in range(num_samples_to_show):\n",
    "        original_text_display = small_train_dataset[i].get('text', 'N/A')\n",
    "        augmented_text_display = augmented_train_dataset[i].get('text', 'N/A')\n",
    "        labels_display = augmented_train_dataset[i].get('labels', 'N/A')\n",
    "        print(f\"Original: {original_text_display}\")\n",
    "        print(f\"Augmented: {augmented_text_display}\")\n",
    "        print(f\"Labels: {labels_display}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Augmented dataset appears empty or has fewer than 5 samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57662428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of augmented training data:\n",
      "Original: My favourite food is anything I didn't have to cook myself.\n",
      "Augmented: My favourite food is anything I didn't have to cook myself.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Augmented: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Labels: [27]\n",
      "\n",
      "Original: WHY THE FUCK IS BAYLESS ISOING\n",
      "Augmented: WHY THE FUCK IS BAYLESS ISOING\n",
      "Labels: [2]\n",
      "\n",
      "Original: To make her feel threatened\n",
      "Augmented: To make her feel threatened\n",
      "Labels: [14]\n",
      "\n",
      "Original: Dirty Southern Wankers\n",
      "Augmented: Dirty Southern Wankers\n",
      "Labels: [3]\n",
      "\n",
      "Original: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Augmented: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Labels: [26]\n",
      "\n",
      "Original: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Augmented: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Labels: [15]\n",
      "\n",
      "Original: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Augmented: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Labels: [8, 20]\n",
      "\n",
      "Original: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Augmented: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Labels: [0]\n",
      "\n",
      "Original: It might be linked to the trust factor of your friend.\n",
      "Augmented: It might be linked to the trust factor of your friend.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Augmented: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Labels: [6]\n",
      "\n",
      "Original: Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol \n",
      "Augmented: Aww... she 'll belike number around eventually, I 'm sure she was just envious of [ Gens ]... I imply, what woman wouldn't be! lol\n",
      "Labels: [1, 4]\n",
      "\n",
      "Original: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Augmented: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Labels: [27]\n",
      "\n",
      "Original: R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family.\n",
      "Augmented: R/sleeptrain Power be meter for some sleep preparation. Take a look and try to feel out what's veracious for your house.\n",
      "Labels: [5]\n",
      "\n",
      "Original: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Augmented: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Labels: [3]\n",
      "\n",
      "Original: Shit, I guess I accidentally bought a Pay-Per-View boxing match\n",
      "Augmented: Shite, I judge I accidentally corrupt a Pay-Per-View boxing couple\n",
      "Labels: [3, 12]\n",
      "\n",
      "Original: Thank you friend\n",
      "Augmented: Thank you friend\n",
      "Labels: [15]\n",
      "\n",
      "Original: Fucking coward.\n",
      "Augmented: Fucking coward.\n",
      "Labels: [2]\n",
      "\n",
      "Original: that is what retardation looks like\n",
      "Augmented: that is what retardation looks like\n",
      "Labels: [27]\n",
      "\n",
      "Original: Maybe that’s what happened to the great white at Houston zoo\n",
      "Augmented: Maybe that’s what happened to the great white at Houston zoo\n",
      "Labels: [6, 22]\n",
      "\n",
      "Original: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Augmented: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Labels: [6, 9, 27]\n",
      "\n",
      "Original: i got a bump and a bald spot. i feel dumb <3\n",
      "Augmented: i got a bump and a bald patch. i finger dull < 3\n",
      "Labels: [12]\n",
      "\n",
      "Original: You are going to do the dishes now\n",
      "Augmented: You are going to do the dishes now\n",
      "Labels: [27]\n",
      "\n",
      "Original: Slowing things down now\n",
      "Augmented: Slowing things down now\n",
      "Labels: [27]\n",
      "\n",
      "Original: His name has already been released. Just can't post it here.\n",
      "Augmented: His name has already been released. Just can't post it here.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Stupidly stubborn / stubbornly stupid\n",
      "Augmented: Stupidly stubborn / stubbornly stupid\n",
      "Labels: [2]\n",
      "\n",
      "Original: Mine was apparently [NAME] and the giant peach!\n",
      "Augmented: Mine was apparently [NAME] and the giant peach!\n",
      "Labels: [27]\n",
      "\n",
      "Original: I miss them being alive\n",
      "Augmented: I miss them being live\n",
      "Labels: [16, 25]\n",
      "\n",
      "Original: Super, thanks\n",
      "Augmented: Super, thanks\n",
      "Labels: [15]\n",
      "\n",
      "Original: A new study just came out from China that it's actually too late.\n",
      "Augmented: A new bailiwick just derive out from China that it's really too late.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Augmented: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Labels: [2]\n",
      "\n",
      "Original: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Augmented: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Labels: [6]\n",
      "\n",
      "Original: Your aunt has some damn nerve, though!\n",
      "Augmented: Your aunt has some damn nerve, though!\n",
      "Labels: [27]\n",
      "\n",
      "Original: Ok, then what the actual fuck is your plan?\n",
      "Augmented: Ok, then what the actual fuck is your plan?\n",
      "Labels: [2, 7]\n",
      "\n",
      "Original: What does FPTP have to do with the referendum?\n",
      "Augmented: What does FPTP have to do with the referendum?\n",
      "Labels: [6]\n",
      "\n",
      "Original: Happy to be able to help.\n",
      "Augmented: Happy to be able to help.\n",
      "Labels: [17]\n",
      "\n",
      "Original: 18 is hot but very bland, it's just here this blonde lady who is not as hot as blonde launch.\n",
      "Augmented: 18 is hot but very bland, it's just here this blond noblewoman who is not as hot as blond launching.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Famous for his 3-4 Defense\n",
      "Augmented: Famous for his 3-4 Refutation\n",
      "Labels: [0]\n",
      "\n",
      "Original: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Augmented: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Labels: [25]\n",
      "\n",
      "Original: When I feel down I listen to music.\n",
      "Augmented: When I feel down I listen to music.\n",
      "Labels: [27]\n",
      "\n",
      "Original: aw, thanks! I appreciate that! \n",
      "Augmented: aw, thanks! I appreciate that! \n",
      "Labels: [0, 15]\n",
      "\n",
      "Original: Thanks! I love watching him every week\n",
      "Augmented: Thanks! I love watching him every week\n",
      "Labels: [15, 18]\n",
      "\n",
      "Original: I read on a different post that he died shortly after of internal injuries.\n",
      "Augmented: I read on a different post that he died shortly after of internal injuries.\n",
      "Labels: [16, 27]\n",
      "\n",
      "Original: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Augmented: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Labels: [27]\n",
      "\n",
      "Original: It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\n",
      "Augmented: It's screwball how far Photoshop has fare. Underwater bridgework?!! NEVER!!!\n",
      "Labels: [7, 13]\n",
      "\n",
      "Original: I wouldn't let a sweet potato dictate decisions, ever.\n",
      "Augmented: I wouldn't let a fresh spud dictate conclusion, ever.\n",
      "Labels: [10]\n",
      "\n",
      "Original: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Augmented: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Labels: [20]\n",
      "\n",
      "Original: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Augmented: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Labels: [27]\n",
      "\n",
      "Original: Sack, shaft, and tip. The trifecta. \n",
      "Augmented: Sack, shaft, and tip. The trifecta. \n",
      "Labels: [27]\n",
      "\n",
      "Original: I know. My question was if they **used** to compete in T5-TTT2.\n",
      "Augmented: I live. My interrogation was if they * * used * * to vie in T5-TTT2.\n",
      "Labels: [27]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print augmented data\n",
    "print(\"Sample of augmented training data:\")\n",
    "\n",
    "for i in range(50):\n",
    "    print(f\"Original: {small_train_dataset[i]['text']}\")\n",
    "    print(f\"Augmented: {augmented_train_dataset[i]['text']}\")\n",
    "    print(f\"Labels: {augmented_train_dataset[i]['labels']}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d906f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 6979.04 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 38330.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Tokenize the small dataset\n",
    "small_emotions_encoded = {}\n",
    "small_emotions_encoded['train'] = small_emotion_dataset['train'].map(tokenize, batched=True, batch_size=None)\n",
    "small_emotions_encoded['test'] = small_emotion_dataset['test'].map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "print(small_emotions_encoded['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f6d48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 24505.29 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 24083.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing columns: ['labels']\n",
      "Renaming 'multi_hot_labels' to 'labels'\n",
      "Calculating sample weights...\n",
      "Sample weights calculated.\n",
      "Setting dataset format to TensorFlow\n",
      "Creating tf.data.Dataset objects with sample weights...\n",
      "Datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "NUM_CLASSES = 28  # Define the number of classes (matches emotions_id2label)\n",
    "\n",
    "def create_multi_hot_labels(example):\n",
    "    \"\"\"Convert the list of labels into a multi-hot encoded vector.\"\"\"\n",
    "    multi_hot_label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "    if 'labels' in example and isinstance(example['labels'], list) and len(example['labels']) > 0:\n",
    "        for label_id in example['labels']:\n",
    "            if isinstance(label_id, int) and 0 <= label_id < NUM_CLASSES:\n",
    "                multi_hot_label[label_id] = 1.0\n",
    "    example['multi_hot_labels'] = multi_hot_label\n",
    "    return example\n",
    "\n",
    "# Apply the conversion to add the new multi-hot label column\n",
    "small_emotions_encoded['train'] = small_emotions_encoded['train'].map(create_multi_hot_labels)\n",
    "small_emotions_encoded['test'] = small_emotions_encoded['test'].map(create_multi_hot_labels)\n",
    "\n",
    "# Remove the original 'labels' column and any leftover 'label_int'\n",
    "columns_to_remove = [col for col in ['label_int', 'labels'] if col in small_emotions_encoded['train'].features]\n",
    "if columns_to_remove:\n",
    "    print(f\"Removing existing columns: {columns_to_remove}\")\n",
    "    small_emotions_encoded['train'] = small_emotions_encoded['train'].remove_columns(columns_to_remove)\n",
    "    small_emotions_encoded['test'] = small_emotions_encoded['test'].remove_columns(columns_to_remove)\n",
    "\n",
    "# Rename the new column 'multi_hot_labels' to 'labels'\n",
    "if 'multi_hot_labels' in small_emotions_encoded['train'].features:\n",
    "    print(\"Renaming 'multi_hot_labels' to 'labels'\")\n",
    "    small_emotions_encoded['train'] = small_emotions_encoded['train'].rename_column('multi_hot_labels', 'labels')\n",
    "if 'multi_hot_labels' in small_emotions_encoded['test'].features:\n",
    "    small_emotions_encoded['test'] = small_emotions_encoded['test'].rename_column('multi_hot_labels', 'labels')\n",
    "\n",
    "# --- Add Sample Weight Calculation ---\n",
    "print(\"Calculating sample weights...\")\n",
    "# Get all multi-hot labels from the training set as a NumPy array\n",
    "train_labels_np = np.array(small_emotions_encoded['train']['labels'])\n",
    "# Count frequency of each label (column-wise sum)\n",
    "label_counts = np.sum(train_labels_np, axis=0)\n",
    "total_samples = len(train_labels_np)\n",
    "\n",
    "# Calculate weight for each class (inverse frequency, smoothed)\n",
    "class_weights_calc = {}\n",
    "for i in range(NUM_CLASSES):\n",
    "    # Avoid division by zero for labels that might not appear in the subset\n",
    "    count = label_counts[i] if label_counts[i] > 0 else 1\n",
    "    class_weights_calc[i] = total_samples / (NUM_CLASSES * count)\n",
    "\n",
    "# Calculate weight for each sample: max weight of its positive labels\n",
    "sample_weights_np = np.zeros(total_samples, dtype=np.float32)\n",
    "for i in range(total_samples):\n",
    "    sample_label_indices = np.where(train_labels_np[i] == 1.0)[0]\n",
    "    if len(sample_label_indices) > 0:\n",
    "        sample_weights_np[i] = max(class_weights_calc[idx] for idx in sample_label_indices)\n",
    "    else:\n",
    "        # Assign a default weight (e.g., 1.0 or average) for samples with no positive labels\n",
    "        sample_weights_np[i] = 1.0\n",
    "print(\"Sample weights calculated.\")\n",
    "# --- End Sample Weight Calculation ---\n",
    "\n",
    "\n",
    "# Set format to tensorflow\n",
    "feature_cols = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "label_col = \"labels\"\n",
    "cols_to_set_format = feature_cols + [label_col]\n",
    "\n",
    "actual_train_cols = list(small_emotions_encoded['train'].features)\n",
    "actual_test_cols = list(small_emotions_encoded['test'].features)\n",
    "final_train_cols = [col for col in cols_to_set_format if col in actual_train_cols]\n",
    "final_test_cols = [col for col in cols_to_set_format if col in actual_test_cols]\n",
    "\n",
    "if all(col in final_train_cols for col in cols_to_set_format) and \\\n",
    "   all(col in final_test_cols for col in cols_to_set_format):\n",
    "    print(\"Setting dataset format to TensorFlow\")\n",
    "    # Don't set format yet, extract numpy arrays first, then create dataset\n",
    "else:\n",
    "     raise ValueError(f\"Error: Could not find all necessary columns. Train has: {actual_train_cols}, Test has: {actual_test_cols}. Needed: {cols_to_set_format}\")\n",
    "\n",
    "\n",
    "# Extract features and labels as numpy arrays before creating dataset\n",
    "train_features_np = {col: np.array(small_emotions_encoded['train'][col]) for col in feature_cols}\n",
    "train_labels_np = np.array(small_emotions_encoded['train']['labels']) # Already have this from weight calc\n",
    "\n",
    "test_features_np = {col: np.array(small_emotions_encoded['test'][col]) for col in feature_cols}\n",
    "test_labels_np = np.array(small_emotions_encoded['test']['labels'])\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "BATCH_SIZE = 32 # Keep batch size reasonable\n",
    "\n",
    "print(\"Creating tf.data.Dataset objects with sample weights...\")\n",
    "# Modify train_dataset to yield (features, labels, sample_weights)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_features_np, train_labels_np, sample_weights_np)\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(len(sample_weights_np)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) # Add prefetch\n",
    "\n",
    "# Test dataset remains (features, labels)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_features_np, test_labels_np)\n",
    ")\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) # Add prefetch\n",
    "print(\"Datasets created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e067bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        # Change activation to 'sigmoid' for multi-label classification\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Make sure we handle the case when inputs is a dictionary\n",
    "        outputs = self.bert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            token_type_ids=inputs['token_type_ids'],\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce5b9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NUM_CLASSES if not defined globally earlier\n",
    "try:\n",
    "    NUM_CLASSES\n",
    "except NameError:\n",
    "    NUM_CLASSES = 28 # Set default if run out of order\n",
    "\n",
    "# Create a shared emotion prediction function for multi-label output\n",
    "def predict_emotion(text, model, threshold=0.5):\n",
    "    \"\"\"Predict multiple emotions for a given text using the provided model and threshold\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    predictions = model(inputs) # Shape: (1, NUM_CLASSES)\n",
    "\n",
    "    # --- Add this line temporarily ---\n",
    "    # print(f\"Raw probabilities for '{text}': {predictions[0].numpy()}\")\n",
    "    # --- End of added line ---\n",
    "\n",
    "    predicted_labels_indices = tf.where(predictions[0] > threshold).numpy().flatten()\n",
    "    predicted_emotions = []\n",
    "    confidences = []\n",
    "    if len(predicted_labels_indices) > 0:\n",
    "        for index in predicted_labels_indices:\n",
    "            predicted_emotions.append(emotions_id2label[index])\n",
    "            confidences.append(float(predictions[0][index]))\n",
    "    else:\n",
    "        # Optional: If no label passes threshold, predict the highest one or 'neutral'\n",
    "        highest_prob_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "        predicted_emotions.append(emotions_id2label[highest_prob_index])\n",
    "        confidences.append(float(predictions[0][highest_prob_index]))\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': text,\n",
    "        'emotions': predicted_emotions,\n",
    "        'confidences': confidences\n",
    "    }\n",
    "\n",
    "# Define test texts to use for both untrained and trained models\n",
    "test_texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"This makes me really angry.\",\n",
    "    \"I'm feeling very sad and disappointed.\",\n",
    "    \"That's really interesting, tell me more.\",\n",
    "    \"I am both excited and nervous about the presentation.\", # Example with multiple emotions\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76f343",
   "metadata": {},
   "source": [
    "## Analyze Test Texts with Untrained Model\n",
    "\n",
    "Let's first create and test our model before training to establish a baseline. This will show how the model performs with random weights, which we can compare to the fine-tuned model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87469b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with UNTRAINED model (random weights - multi-label):\n",
      "-------------------------------------------------------------\n",
      "Raw probabilities for 'I'm so happy today!': [0.64240456 0.7607768  0.41271386 0.17730935 0.18901902 0.5629299\n",
      " 0.2589481  0.74051845 0.30491742 0.36972788 0.62420976 0.6385397\n",
      " 0.773479   0.5157682  0.4701345  0.2500274  0.7306917  0.6394449\n",
      " 0.36029348 0.8246309  0.55854404 0.5302149  0.76650345 0.37005234\n",
      " 0.3335643  0.5530919  0.6025893  0.42961365]\n",
      "Text: I'm so happy today!\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6424045562744141), ('amusement', 0.7607768177986145), ('anger', 0.4127138555049896), ('annoyance', 0.17730934917926788), ('approval', 0.18901902437210083), ('caring', 0.5629299283027649), ('confusion', 0.25894808769226074), ('curiosity', 0.7405184507369995), ('desire', 0.30491742491722107), ('disappointment', 0.36972787976264954), ('disapproval', 0.6242097616195679), ('disgust', 0.6385396718978882), ('embarrassment', 0.7734789848327637), ('excitement', 0.5157681703567505), ('fear', 0.4701344966888428), ('gratitude', 0.2500273883342743), ('grief', 0.73069167137146), ('joy', 0.639444887638092), ('love', 0.3602934777736664), ('nervousness', 0.8246309161186218), ('optimism', 0.5585440397262573), ('pride', 0.5302149057388306), ('realization', 0.7665034532546997), ('relief', 0.3700523376464844), ('remorse', 0.33356431126594543), ('sadness', 0.5530918836593628), ('surprise', 0.6025893092155457), ('neutral', 0.4296136498451233)]\n",
      "\n",
      "Raw probabilities for 'This makes me really angry.': [0.7105702  0.4495927  0.30021673 0.12030686 0.22675344 0.66601413\n",
      " 0.24661225 0.71624833 0.25794938 0.34615096 0.847026   0.515566\n",
      " 0.7549007  0.5098272  0.5870253  0.14359905 0.74130607 0.79984975\n",
      " 0.64645827 0.8397953  0.5237779  0.24103284 0.6996865  0.509712\n",
      " 0.19584928 0.55426836 0.55354303 0.2610282 ]\n",
      "Text: This makes me really angry.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.710570216178894), ('amusement', 0.4495927095413208), ('anger', 0.3002167344093323), ('annoyance', 0.12030685693025589), ('approval', 0.22675344347953796), ('caring', 0.6660141348838806), ('confusion', 0.24661225080490112), ('curiosity', 0.7162483334541321), ('desire', 0.2579493820667267), ('disappointment', 0.3461509644985199), ('disapproval', 0.847025990486145), ('disgust', 0.5155659914016724), ('embarrassment', 0.7549006938934326), ('excitement', 0.509827196598053), ('fear', 0.5870252847671509), ('gratitude', 0.14359904825687408), ('grief', 0.7413060665130615), ('joy', 0.7998497486114502), ('love', 0.6464582681655884), ('nervousness', 0.8397952914237976), ('optimism', 0.5237779021263123), ('pride', 0.24103283882141113), ('realization', 0.6996865272521973), ('relief', 0.5097119808197021), ('remorse', 0.19584928452968597), ('sadness', 0.5542683601379395), ('surprise', 0.5535430312156677), ('neutral', 0.2610282003879547)]\n",
      "\n",
      "Raw probabilities for 'I'm feeling very sad and disappointed.': [0.69302386 0.5497563  0.31268802 0.14394036 0.17792343 0.5694339\n",
      " 0.18059951 0.78020424 0.32472548 0.3470153  0.79235536 0.5568674\n",
      " 0.78668785 0.47127572 0.48803985 0.18490036 0.72877246 0.7233436\n",
      " 0.4803924  0.81477314 0.5063062  0.3732838  0.732281   0.41234818\n",
      " 0.27347484 0.63971823 0.6020157  0.38929844]\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6930238604545593), ('amusement', 0.5497562885284424), ('anger', 0.31268802285194397), ('annoyance', 0.14394035935401917), ('approval', 0.17792342603206635), ('caring', 0.5694339275360107), ('confusion', 0.18059951066970825), ('curiosity', 0.7802042365074158), ('desire', 0.324725478887558), ('disappointment', 0.34701529145240784), ('disapproval', 0.7923553586006165), ('disgust', 0.5568674206733704), ('embarrassment', 0.7866878509521484), ('excitement', 0.4712757170200348), ('fear', 0.48803985118865967), ('gratitude', 0.18490035831928253), ('grief', 0.7287724614143372), ('joy', 0.7233436107635498), ('love', 0.4803923964500427), ('nervousness', 0.8147731423377991), ('optimism', 0.5063061714172363), ('pride', 0.3732838034629822), ('realization', 0.7322810292243958), ('relief', 0.4123481810092926), ('remorse', 0.2734748423099518), ('sadness', 0.639718234539032), ('surprise', 0.6020156741142273), ('neutral', 0.3892984390258789)]\n",
      "\n",
      "Raw probabilities for 'That's really interesting, tell me more.': [0.6673931  0.68322885 0.39065474 0.17721848 0.20334406 0.5678425\n",
      " 0.24359901 0.68713826 0.2411537  0.4063151  0.74501145 0.6537101\n",
      " 0.742376   0.5087035  0.54594463 0.20996797 0.7202012  0.68980867\n",
      " 0.44109362 0.88491803 0.4915155  0.41533867 0.74841344 0.42702708\n",
      " 0.28181228 0.60828257 0.54636323 0.43637398]\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6673930883407593), ('amusement', 0.6832288503646851), ('anger', 0.3906547427177429), ('annoyance', 0.1772184818983078), ('approval', 0.20334406197071075), ('caring', 0.5678424835205078), ('confusion', 0.24359901249408722), ('curiosity', 0.6871382594108582), ('desire', 0.24115370213985443), ('disappointment', 0.4063150882720947), ('disapproval', 0.7450114488601685), ('disgust', 0.653710126876831), ('embarrassment', 0.7423760294914246), ('excitement', 0.5087034702301025), ('fear', 0.5459446310997009), ('gratitude', 0.2099679708480835), ('grief', 0.7202011942863464), ('joy', 0.6898086667060852), ('love', 0.4410936236381531), ('nervousness', 0.8849180340766907), ('optimism', 0.49151548743247986), ('pride', 0.4153386652469635), ('realization', 0.7484134435653687), ('relief', 0.4270270764827728), ('remorse', 0.28181228041648865), ('sadness', 0.6082825660705566), ('surprise', 0.5463632345199585), ('neutral', 0.4363739788532257)]\n",
      "\n",
      "Raw probabilities for 'I am both excited and nervous about the presentation.': [0.70417804 0.5417714  0.35355198 0.12004349 0.15688144 0.60557324\n",
      " 0.20196846 0.79114014 0.37523136 0.3252459  0.78376293 0.5220251\n",
      " 0.7642598  0.4508476  0.45240316 0.20666341 0.76410455 0.76305735\n",
      " 0.5087837  0.7735311  0.5146118  0.41281584 0.7219109  0.36448473\n",
      " 0.26890585 0.6005785  0.60714704 0.33757466]\n",
      "Text: I am both excited and nervous about the presentation.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.7041780352592468), ('amusement', 0.541771411895752), ('anger', 0.353551983833313), ('annoyance', 0.12004348635673523), ('approval', 0.1568814367055893), ('caring', 0.6055732369422913), ('confusion', 0.2019684612751007), ('curiosity', 0.7911401391029358), ('desire', 0.3752313554286957), ('disappointment', 0.3252458870410919), ('disapproval', 0.7837629318237305), ('disgust', 0.5220251083374023), ('embarrassment', 0.7642598152160645), ('excitement', 0.4508475959300995), ('fear', 0.45240315794944763), ('gratitude', 0.20666341483592987), ('grief', 0.7641045451164246), ('joy', 0.7630573511123657), ('love', 0.5087836980819702), ('nervousness', 0.7735310792922974), ('optimism', 0.5146117806434631), ('pride', 0.4128158390522003), ('realization', 0.7219108939170837), ('relief', 0.3644847273826599), ('remorse', 0.2689058482646942), ('sadness', 0.6005784869194031), ('surprise', 0.6071470379829407), ('neutral', 0.33757466077804565)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model for baseline comparison\n",
    "# Ensure the base model 'model' is loaded correctly from cell 2\n",
    "untrained_classifier = BERTForClassification(model, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compile the model for multi-label classification\n",
    "untrained_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5),\n",
    "    # Use BinaryCrossentropy for multi-label with sigmoid activation\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    # Use BinaryAccuracy for multi-label evaluation\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "print(\"Predictions with UNTRAINED model (random weights - multi-label):\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Get predictions from untrained model using our updated shared function\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, untrained_classifier, threshold=0.1) # Lower threshold for untrained might show more random outputs\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotions: {result['emotions']}\")\n",
    "    # Zip confidences with emotions for clarity\n",
    "    emotion_confidence_pairs = list(zip(result['emotions'], result['confidences']))\n",
    "    print(f\"Confidences: {emotion_confidence_pairs}\")\n",
    "    # print(f\"Confidences: {[f'{c:.4f}' for c in result['confidences']]}\")\n",
    "    print()\n",
    "\n",
    "# Evaluating accuracy on the test set for an untrained multi-label model isn't very informative\n",
    "# untrained_loss, untrained_accuracy = untrained_classifier.evaluate(test_dataset, verbose=0)\n",
    "# print(f\"Untrained model test accuracy (BinaryAccuracy): {untrained_accuracy:.4f}\")\n",
    "# Random baseline for BinaryAccuracy depends on label distribution, harder to interpret than single-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d127d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model with AUC, Precision, Recall...\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Suggested Change for Cell ID: d127d7b4 ---\n",
    "\n",
    "# Update num_classes if not already defined\n",
    "try:\n",
    "    NUM_CLASSES\n",
    "except NameError:\n",
    "    NUM_CLASSES = 28\n",
    "\n",
    "# Define the model - ensure 'model' (the base BERT model) is loaded\n",
    "classifier = BERTForClassification(model, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compile the model for multi-label classification with more metrics\n",
    "print(\"Compiling model with AUC, Precision, Recall...\")\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5), # Consider trying AdamW later\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), # Correct loss for multi-label sigmoid\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(multi_label=True, name='auc'), # Good overall multi-label metric\n",
    "        tf.keras.metrics.Precision(name='precision'), # How many selected items are relevant?\n",
    "        tf.keras.metrics.Recall(name='recall') # How many relevant items are selected?\n",
    "        ]\n",
    ")\n",
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9df6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-label model training with sample weights...\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 720s 2s/step - loss: 0.1865 - accuracy: 0.9510 - auc: 0.5882 - precision: 0.0749 - recall: 0.0141 - val_loss: 0.1423 - val_accuracy: 0.9610 - val_auc: 0.7653 - val_precision: 0.9571 - val_recall: 0.0579\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 741s 2s/step - loss: 0.1367 - accuracy: 0.9604 - auc: 0.8260 - precision: 0.8304 - recall: 0.0787 - val_loss: 0.1210 - val_accuracy: 0.9631 - val_auc: 0.8775 - val_precision: 0.7541 - val_recall: 0.1590\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 713s 2s/step - loss: 0.1108 - accuracy: 0.9633 - auc: 0.8965 - precision: 0.7561 - recall: 0.1926 - val_loss: 0.1120 - val_accuracy: 0.9642 - val_auc: 0.8926 - val_precision: 0.7273 - val_recall: 0.2143\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 731s 2s/step - loss: 0.0905 - accuracy: 0.9652 - auc: 0.9333 - precision: 0.7311 - recall: 0.2792 - val_loss: 0.1090 - val_accuracy: 0.9641 - val_auc: 0.9009 - val_precision: 0.6396 - val_recall: 0.2990\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 734s 2s/step - loss: 0.0773 - accuracy: 0.9676 - auc: 0.9498 - precision: 0.7283 - recall: 0.3700 - val_loss: 0.1068 - val_accuracy: 0.9629 - val_auc: 0.9099 - val_precision: 0.5896 - val_recall: 0.3328\n",
      "Training finished.\n",
      "Evaluating model on test set...\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.1068 - accuracy: 0.9629 - auc: 0.9099 - precision: 0.5896 - recall: 0.3328\n",
      "\n",
      "Test Set Evaluation Results:\n",
      "- loss: 0.1068\n",
      "- accuracy: 0.9629\n",
      "- auc: 0.9099\n",
      "- precision: 0.5896\n",
      "- recall: 0.3328\n"
     ]
    }
   ],
   "source": [
    "# --- Suggested Change for Cell ID: e9df6074 ---\n",
    "\n",
    "# Train the model\n",
    "# Sample weights are now included in train_dataset, so no class_weight argument needed\n",
    "# Ensure train_dataset and test_dataset are correctly defined from the previous cell\n",
    "\n",
    "print(\"Starting multi-label model training with sample weights...\")\n",
    "# Consider adding callbacks like EarlyStopping or ModelCheckpoint for longer runs\n",
    "# callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=3, mode='max', restore_best_weights=True),\n",
    "#     tf.keras.callbacks.ModelCheckpoint('best_emotion_model.keras', save_best_only=True, monitor='val_auc', mode='max')\n",
    "# ]\n",
    "\n",
    "history = classifier.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,  # Adjust epochs as needed, more data might require more/fewer epochs\n",
    "    validation_data=test_dataset\n",
    "    # callbacks=callbacks # Uncomment to use callbacks\n",
    ")\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = classifier.evaluate(test_dataset, verbose=1) # Use verbose=1 to see progress\n",
    "\n",
    "# Print evaluation results dynamically based on compiled metrics\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for name, value in zip(classifier.metrics_names, results):\n",
    "    print(f\"- {name}: {value:.4f}\")\n",
    "\n",
    "# Example: Accessing specific metrics if needed\n",
    "# test_loss = results[classifier.metrics_names.index('loss')]\n",
    "# test_auc = results[classifier.metrics_names.index('auc')]\n",
    "# print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f61dbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with TRAINED multi-label model:\n",
      "----------------------------------------\n",
      "Text: I'm so happy today!\n",
      "Predicted emotions: ['excitement', 'joy']\n",
      "Confidences: [('excitement', 0.12582339346408844), ('joy', 0.7270192503929138)]\n",
      "\n",
      "Text: This makes me really angry.\n",
      "Predicted emotions: ['anger', 'annoyance']\n",
      "Confidences: [('anger', 0.702703595161438), ('annoyance', 0.11748744547367096)]\n",
      "\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotions: ['disappointment', 'grief', 'sadness']\n",
      "Confidences: [('disappointment', 0.3193146586418152), ('grief', 0.1527215540409088), ('sadness', 0.8931897878646851)]\n",
      "\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotions: ['excitement', 'joy']\n",
      "Confidences: [('excitement', 0.731715202331543), ('joy', 0.12100547552108765)]\n",
      "\n",
      "Text: I am both excited and nervous about the presentation.\n",
      "Predicted emotions: ['fear', 'nervousness']\n",
      "Confidences: [('fear', 0.31013643741607666), ('nervousness', 0.45450836420059204)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions with TRAINED multi-label model:\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Use the updated shared function with the trained model\n",
    "prediction_threshold = 0.1 # Adjust threshold as needed based on validation performance\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, classifier, threshold=prediction_threshold)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotions: {result['emotions']}\")\n",
    "    # Zip confidences with emotions for clarity\n",
    "    emotion_confidence_pairs = list(zip(result['emotions'], result['confidences']))\n",
    "    print(f\"Confidences: {emotion_confidence_pairs}\")\n",
    "    # print(f\"Confidences: {[f'{c:.4f}' for c in result['confidences']]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
