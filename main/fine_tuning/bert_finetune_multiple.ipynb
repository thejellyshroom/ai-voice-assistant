{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb4c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcfc88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca87985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "emotion_dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5c1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a larger subset of examples for better training\n",
    "small_train_dataset = emotion_dataset['train'].select(range(10000))\n",
    "small_test_dataset = emotion_dataset['test'].select(range(1000))\n",
    "\n",
    "# Create a new small dataset with the reduced splits\n",
    "small_emotion_dataset = {\n",
    "    'train': small_train_dataset,\n",
    "    'test': small_test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe6e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of emotions dataset:\n",
      "{'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [27], 'id': 'eebbqej'}\n",
      "Number of labels: 28\n"
     ]
    }
   ],
   "source": [
    "emotions_id2label = {\n",
    "    0: 'admiration',\n",
    "    1: 'amusement',\n",
    "    2: 'anger',\n",
    "    3: 'annoyance',\n",
    "    4: 'approval',\n",
    "    5: 'caring',\n",
    "    6: 'confusion',\n",
    "    7: 'curiosity',\n",
    "    8: 'desire',\n",
    "    9: 'disappointment',\n",
    "    10: 'disapproval',\n",
    "    11: 'disgust',\n",
    "    12: 'embarrassment',\n",
    "    13: 'excitement',\n",
    "    14: 'fear',\n",
    "    15: 'gratitude',\n",
    "    16: 'grief',\n",
    "    17: 'joy',\n",
    "    18: 'love',\n",
    "    19: 'nervousness',\n",
    "    20: 'optimism',\n",
    "    21: 'pride',\n",
    "    22: 'realization',\n",
    "    23: 'relief',\n",
    "    24: 'remorse',\n",
    "    25: 'sadness',\n",
    "    26: 'surprise',\n",
    "    27: 'neutral'  # Last entry (no comma)\n",
    "}\n",
    "\n",
    "emotions_label2id = {v: k for k, v in emotions_id2label.items()}\n",
    "\n",
    "# Print dataset info to verify we understand what we're working with\n",
    "print(\"Sample of emotions dataset:\")\n",
    "print(small_emotion_dataset[\"train\"][0])\n",
    "print(f\"Number of labels: {len(emotions_id2label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c515f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing original training data for minority classes...\n",
      "Identified 5 minority classes (frequency < 1.0%, count < 100):\n",
      "Minority Classes Indices: {12, 16, 19, 21, 23}\n",
      "Minority Classes Names: {'pride', 'grief', 'relief', 'embarrassment', 'nervousness'}\n",
      "Applying data augmentation to training set (this may take a while)...\n",
      "Augmentation complete.\n",
      "Sample of augmented training data:\n",
      "Original: My favourite food is anything I didn't have to cook myself.\n",
      "Augmented: My favourite food is anything I didn't have to cook myself.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Augmented: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Labels: [27]\n",
      "\n",
      "Original: WHY THE FUCK IS BAYLESS ISOING\n",
      "Augmented: WHY THE FUCK IS BAYLESS ISOING\n",
      "Labels: [2]\n",
      "\n",
      "Original: To make her feel threatened\n",
      "Augmented: To make her feel threatened\n",
      "Labels: [14]\n",
      "\n",
      "Original: Dirty Southern Wankers\n",
      "Augmented: Dirty Southern Wankers\n",
      "Labels: [3]\n",
      "\n",
      "Original: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Augmented: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Labels: [26]\n",
      "\n",
      "Original: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Augmented: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Labels: [15]\n",
      "\n",
      "Original: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Augmented: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Labels: [8, 20]\n",
      "\n",
      "Original: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Augmented: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Labels: [0]\n",
      "\n",
      "Original: It might be linked to the trust factor of your friend.\n",
      "Augmented: It might be linked to the trust factor of your friend.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Augmented: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Labels: [6]\n",
      "\n",
      "Original: Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol \n",
      "Augmented: Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol \n",
      "Labels: [1, 4]\n",
      "\n",
      "Original: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Augmented: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Labels: [27]\n",
      "\n",
      "Original: R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family.\n",
      "Augmented: R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family.\n",
      "Labels: [5]\n",
      "\n",
      "Original: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Augmented: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Labels: [3]\n",
      "\n",
      "Original: Shit, I guess I accidentally bought a Pay-Per-View boxing match\n",
      "Augmented: Shit, I guess I incidentally buy a Pay-Per-View fisticuffs lucifer\n",
      "Labels: [3, 12]\n",
      "\n",
      "Original: Thank you friend\n",
      "Augmented: Thank you friend\n",
      "Labels: [15]\n",
      "\n",
      "Original: Fucking coward.\n",
      "Augmented: Fucking coward.\n",
      "Labels: [2]\n",
      "\n",
      "Original: that is what retardation looks like\n",
      "Augmented: that is what slowing looks like\n",
      "Labels: [27]\n",
      "\n",
      "Original: Maybe that’s what happened to the great white at Houston zoo\n",
      "Augmented: Maybe that’s what happened to the great white at Houston zoo\n",
      "Labels: [6, 22]\n",
      "\n",
      "Original: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Augmented: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Labels: [6, 9, 27]\n",
      "\n",
      "Original: i got a bump and a bald spot. i feel dumb <3\n",
      "Augmented: i got a bump and a bald smirch. i palpate speechless < 3\n",
      "Labels: [12]\n",
      "\n",
      "Original: You are going to do the dishes now\n",
      "Augmented: You are going to do the peach now\n",
      "Labels: [27]\n",
      "\n",
      "Original: Slowing things down now\n",
      "Augmented: Slowing things down now\n",
      "Labels: [27]\n",
      "\n",
      "Original: His name has already been released. Just can't post it here.\n",
      "Augmented: His name has already been released. Just can't post it here.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Stupidly stubborn / stubbornly stupid\n",
      "Augmented: Stupidly stubborn / stubbornly stupid\n",
      "Labels: [2]\n",
      "\n",
      "Original: Mine was apparently [NAME] and the giant peach!\n",
      "Augmented: Mine was apparently [NAME] and the giant peach!\n",
      "Labels: [27]\n",
      "\n",
      "Original: I miss them being alive\n",
      "Augmented: I drop them being alive\n",
      "Labels: [16, 25]\n",
      "\n",
      "Original: Super, thanks\n",
      "Augmented: Superintendent, thanks\n",
      "Labels: [15]\n",
      "\n",
      "Original: A new study just came out from China that it's actually too late.\n",
      "Augmented: A new study just came out from China that it's actually too late.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Augmented: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Labels: [2]\n",
      "\n",
      "Original: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Augmented: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Labels: [6]\n",
      "\n",
      "Original: Your aunt has some damn nerve, though!\n",
      "Augmented: Your aunt has some damn nerve, though!\n",
      "Labels: [27]\n",
      "\n",
      "Original: Ok, then what the actual fuck is your plan?\n",
      "Augmented: Ok, then what the actual fuck is your plan?\n",
      "Labels: [2, 7]\n",
      "\n",
      "Original: What does FPTP have to do with the referendum?\n",
      "Augmented: What does FPTP have to do with the referendum?\n",
      "Labels: [6]\n",
      "\n",
      "Original: Happy to be able to help.\n",
      "Augmented: Happy to be able to help.\n",
      "Labels: [17]\n",
      "\n",
      "Original: 18 is hot but very bland, it's just here this blonde lady who is not as hot as blonde launch.\n",
      "Augmented: 18 is hot but very bland, it's just here this blonde lady who is not as hot as blonde launch.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Famous for his 3-4 Defense\n",
      "Augmented: Famous for his 3-4 Vindication\n",
      "Labels: [0]\n",
      "\n",
      "Original: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Augmented: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Labels: [25]\n",
      "\n",
      "Original: When I feel down I listen to music.\n",
      "Augmented: When I feel down I listen to music.\n",
      "Labels: [27]\n",
      "\n",
      "Original: aw, thanks! I appreciate that! \n",
      "Augmented: aw, thanks! I appreciate that! \n",
      "Labels: [0, 15]\n",
      "\n",
      "Original: Thanks! I love watching him every week\n",
      "Augmented: Thanks! I know determine him every week\n",
      "Labels: [15, 18]\n",
      "\n",
      "Original: I read on a different post that he died shortly after of internal injuries.\n",
      "Augmented: I show on a unlike office that he died curtly after of internal injuries.\n",
      "Labels: [16, 27]\n",
      "\n",
      "Original: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Augmented: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Labels: [27]\n",
      "\n",
      "Original: It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\n",
      "Augmented: It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\n",
      "Labels: [7, 13]\n",
      "\n",
      "Original: I wouldn't let a sweet potato dictate decisions, ever.\n",
      "Augmented: I wouldn't let a mellisonant murphy dictate decisions, always.\n",
      "Labels: [10]\n",
      "\n",
      "Original: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Augmented: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Labels: [20]\n",
      "\n",
      "Original: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Augmented: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Labels: [27]\n",
      "\n",
      "Original: Sack, shaft, and tip. The trifecta. \n",
      "Augmented: Sack, shaft, and tip. The trifecta. \n",
      "Labels: [27]\n",
      "\n",
      "Original: I know. My question was if they **used** to compete in T5-TTT2.\n",
      "Augmented: I know. My question was if they **used** to compete in T5-TTT2.\n",
      "Labels: [27]\n",
      "\n",
      "Original: FBI!! OPEN UP!!!\n",
      "Augmented: FBI!! OPEN UP!!!\n",
      "Labels: [27]\n",
      "\n",
      "Original: Should’ve dumped coke all over her right after the movie, and make a run for it. Fk pettiness \n",
      "Augmented: Should’ve dumped coke all over her right after the movie, and make a run for it. Fk pettiness \n",
      "Labels: [27]\n",
      "\n",
      "Original: You can always kneel.\n",
      "Augmented: You can always kneel.\n",
      "Labels: [4]\n",
      "\n",
      "Original: Cheers, sololander!\n",
      "Augmented: Cheers, sololander!\n",
      "Labels: [27]\n",
      "\n",
      "Original: Very interesting. Thx\n",
      "Augmented: Very interesting. Thx\n",
      "Labels: [13, 15]\n",
      "\n",
      "Original: This isn't really wholesome\n",
      "Augmented: This isn't really wholesome\n",
      "Labels: [10]\n",
      "\n",
      "Original: your mom likes to copy me cause she has no creativity just like you:\n",
      "Augmented: your mom likes to copy me cause she has no creativity just like you:\n",
      "Labels: [27]\n",
      "\n",
      "Original: Lord help me I want to be a mod again. So many damn trolls here\n",
      "Augmented: Lord help me I want to be a mod again. So many damn trolls here\n",
      "Labels: [27]\n",
      "\n",
      "Original: Oh holy heck :/\n",
      "Augmented: Oh holy heck :/\n",
      "Labels: [27]\n",
      "\n",
      "Original: Thanks DB, I'll see if I can find the book\n",
      "Augmented: Thanks DB, I'll see if I can find the book\n",
      "Labels: [15]\n",
      "\n",
      "Original: LOL. Super cute!\n",
      "Augmented: LOL. Super cute!\n",
      "Labels: [0, 1]\n",
      "\n",
      "Original: I just shit my pants. Then walk away. Embarrassing enough he won't press or follow you.\n",
      "Augmented: I just denounce my gasp. Then walk forth. Abash enough he won't press or postdate you.\n",
      "Labels: [12]\n",
      "\n",
      "Original: WHAT ARE YOU DOIN [NAME]?\n",
      "Augmented: WHAT ARE YOU DOIN [NAME]?\n",
      "Labels: [27]\n",
      "\n",
      "Original: This...has 9k upvotes. Wow.\n",
      "Augmented: This...has 9k upvotes. Wow.\n",
      "Labels: [13]\n",
      "\n",
      "Original: [NAME] sees all\n",
      "Augmented: [NAME] sees all\n",
      "Labels: [27]\n",
      "\n",
      "Original: Awesome. Thanks!\n",
      "Augmented: Awesome. Thanks!\n",
      "Labels: [0, 15]\n",
      "\n",
      "Original: Wait. What. How?\n",
      "Augmented: Wait. What. How?\n",
      "Labels: [27]\n",
      "\n",
      "Original: And then they say, “HAHAHAHHA IT WAS RIGHT THERE WOW!”\n",
      "Augmented: And then they say, “HAHAHAHHA IT WAS RIGHT THERE WOW!”\n",
      "Labels: [1]\n",
      "\n",
      "Original: Aww, try mindfulness. I think I am going over to that sub now.\n",
      "Augmented: Aww, try mindfulness. I think I am going over to that sub now.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Twilight... STILL a better love story than The Last Jedi!\n",
      "Augmented: Twilight... STILL a better love story than The Last Jedi!\n",
      "Labels: [0]\n",
      "\n",
      "Original: Detective from svu.\n",
      "Augmented: Detective from svu.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Good for #70 to console the poor guy\n",
      "Augmented: Good for #70 to console the poor guy\n",
      "Labels: [0, 5]\n",
      "\n",
      "Original: The republicans are the military. You are an idiot.\n",
      "Augmented: The republican are the military. You are an idiot.\n",
      "Labels: [3]\n",
      "\n",
      "Original: Don't kiss your doorbell! Or anyone else's for that matter...\n",
      "Augmented: Don't kiss your doorbell! Or anyone else's for that matter...\n",
      "Labels: [27]\n",
      "\n",
      "Original: Just got home from school. How are we doing\n",
      "Augmented: Just got home from school. How are we doing\n",
      "Labels: [27]\n",
      "\n",
      "Original: This pic they used for [NAME] makes her look like [NAME]\n",
      "Augmented: This pic they utilize for [ Gens ] do her look like [ NAME ]\n",
      "Labels: [27]\n",
      "\n",
      "Original: Awesome! I’m a cradle [RELIGION], so really interesting to hear your experience. Thanks for sharing.\n",
      "Augmented: Awesome! I’m a cradle [RELIGION], so really interesting to hear your experience. Thanks for sharing.\n",
      "Labels: [0, 13, 15]\n",
      "\n",
      "Original: What a wonderful world\n",
      "Augmented: What a wonderful world\n",
      "Labels: [0]\n",
      "\n",
      "Original: Never get out of the boat.\n",
      "Augmented: Never get out of the boat.\n",
      "Labels: [27]\n",
      "\n",
      "Original: just noticed, lol. damn pervert foreigners.\n",
      "Augmented: just noticed, lol. damn pervert foreigners.\n",
      "Labels: [1]\n",
      "\n",
      "Original: Oh forgive us for trying to make an exciting atmosphere at our homecourt.\n",
      "Augmented: Oh forgive us for trying to make an exciting atmosphere at our homecourt.\n",
      "Labels: [13]\n",
      "\n",
      "Original: true I am a troll, but fortunately for me I'm not emotionally invested in it.\n",
      "Augmented: true I am a troll, but fortunately for me I'm not emotionally invested in it.\n",
      "Labels: [4]\n",
      "\n",
      "Original: sorry [NAME]! 😘😘😘\n",
      "Augmented: sorry [NAME]! 😘😘😘\n",
      "Labels: [25]\n",
      "\n",
      "Original: At least it’s not anything worse, and that you are still close to that person :)\n",
      "Augmented: At least it’s not anything worse, and that you are still close to that person :)\n",
      "Labels: [4]\n",
      "\n",
      "Original: This is the first person Australia has tried to do this to...\n",
      "Augmented: This is the first person Australia has tried to do this to...\n",
      "Labels: [27]\n",
      "\n",
      "Original: my brain hurts...\n",
      "Augmented: my encephalon hurts...\n",
      "Labels: [25]\n",
      "\n",
      "Original: Thank you for at least doing something good sir.\n",
      "Augmented: Thank you for at least doing something good sir.\n",
      "Labels: [0, 15]\n",
      "\n",
      "Original: He was off by 5 minutes, not impressed. \n",
      "Augmented: He was off by 5 minutes, not impressed. \n",
      "Labels: [9]\n",
      "\n",
      "Original: Sometimes life actually hands you lemons. We're just lucky that we have a proverbial phrase that gives us an idea of what we can do with them.\n",
      "Augmented: Sometimes life really reach you stinker. We 're just lucky that we have a proverbial idiom that collapse us an idea of what we can do with them.\n",
      "Labels: [4, 22]\n",
      "\n",
      "Original: People on the other side want you dead no matter why you are there.\n",
      "Augmented: People on the other side want you dead no matter why you are there.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Pay you for what, just standing there? Done.\n",
      "Augmented: Pay you for what, just standing there? Done.\n",
      "Labels: [4]\n",
      "\n",
      "Original: Cleavage: Nature's Brass Catcher.\n",
      "Augmented: Cleavage: Nature's Brass Catcher.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Apologies, I take it all back as I’ve just seen his latest effort\n",
      "Augmented: Apologies, I take it all back as I’ve just seen his latest effort\n",
      "Labels: [24]\n",
      "\n",
      "Original: I love Rocket Love and Blasted. I just wonder who the songs were written for because these are all reference tracks except Acura Intergul\n",
      "Augmented: I jazz Roquette Honey and Blasted. I just wonder who the call were written for because these are all reference track except Acura Intergul\n",
      "Labels: [18]\n",
      "\n",
      "Original: okay im interested in joining the bare hands hunting posse\n",
      "Augmented: fine im interested in connect the bare reach run posse\n",
      "Labels: [4]\n",
      "\n",
      "Original: Damn for that much you may as well use a mobile plan unless you're doing something latency sensitive.\n",
      "Augmented: Damn for that much you may as well use a mobile plan unless you're doing something latency sensitive.\n",
      "Labels: [27]\n",
      "\n",
      "Original: I think the 90 day rule applies to increases over 5%?\n",
      "Augmented: I think the 90 day rule applies to increases over 5%?\n",
      "Labels: [7]\n",
      "\n",
      "Original: If they bolt its a feature.\n",
      "Augmented: If they bolt its a feature.\n",
      "Labels: [27]\n",
      "\n",
      "Original: So this means the people who have debt can see those that don’t. Am I sensing an easier target for muggings and such?\n",
      "Augmented: So this means the people who have debt can see those that don’t. Am I sensing an easier target for muggings and such?\n",
      "Labels: [7]\n",
      "\n",
      "Original: Skynet has really lost its edge\n",
      "Augmented: Skynet has really lost its edge\n",
      "Labels: [27]\n",
      "\n",
      "Applying data augmentation to training set (this may take a while)...\n",
      "Augmentation complete.\n"
     ]
    }
   ],
   "source": [
    "MINORITY_THRESHOLD_PERCENT = 1.0  # Set the threshold for minority classes (e.g., 1% of total samples)\n",
    "from collections import Counter\n",
    "from utils import augment_data\n",
    "\n",
    "\n",
    "print(\"Analyzing original training data for minority classes...\")\n",
    "original_train_labels = small_train_dataset['labels']\n",
    "all_labels = [label for sublist in original_train_labels for label in sublist]\n",
    "label_counts_original = Counter(all_labels)\n",
    "total_samples_original = len(small_train_dataset)\n",
    "\n",
    "# --- Identify minority classes (e.g., frequency < 1% of total samples) ---\n",
    "minority_threshold_count = total_samples_original * (MINORITY_THRESHOLD_PERCENT / 100.0)\n",
    "minority_classes = {\n",
    "    label for label, count in label_counts_original.items()\n",
    "    if count < minority_threshold_count\n",
    "}\n",
    "if minority_classes:\n",
    "    print(f\"Identified {len(minority_classes)} minority classes (frequency < {MINORITY_THRESHOLD_PERCENT}%, count < {minority_threshold_count:.0f}):\")\n",
    "    minority_class_names = {emotions_id2label.get(idx, f\"Unknown({idx})\") for idx in minority_classes}\n",
    "    print(f\"Minority Classes Indices: {minority_classes}\")\n",
    "    print(f\"Minority Classes Names: {minority_class_names}\")\n",
    "else:\n",
    "    print(f\"No minority classes found with frequency < {MINORITY_THRESHOLD_PERCENT}%.\")\n",
    "\n",
    "print(\"Applying data augmentation to training set (this may take a while)...\")\n",
    "# Use functools.partial to pass fixed arguments to augment_data\n",
    "from functools import partial\n",
    "augment_fn = partial(augment_data,\n",
    "                     minority_classes_set=minority_classes,)\n",
    "\n",
    "augmented_train_dataset = small_train_dataset.map(augment_fn, num_proc=1) # Start with 1 process\n",
    "print(\"Augmentation complete.\")\n",
    "\n",
    "# print augmented data\n",
    "print(\"Sample of augmented training data:\")\n",
    "for i in range(100):\n",
    "    print(f\"Original: {small_train_dataset[i]['text']}\")\n",
    "    print(f\"Augmented: {augmented_train_dataset[i]['text']}\")\n",
    "    print(f\"Labels: {augmented_train_dataset[i]['labels']}\")\n",
    "    print()\n",
    "# --- End Augmentation Application ---\n",
    "\n",
    "\n",
    "# --- Apply Augmentation ONLY to Training Data ---\n",
    "print(\"Applying data augmentation to training set (this may take a while)...\")\n",
    "# Use functools.partial to pass fixed arguments to augment_data\n",
    "from functools import partial\n",
    "augment_fn = partial(augment_data,\n",
    "                     target_minority=True,\n",
    "                     minority_classes_set=minority_classes,\n",
    "                     base_prob=0.15, # Adjust base probability as needed\n",
    "                     minority_boost_prob=0.4) # Adjust boost probability as needed\n",
    "\n",
    "# Consider using num_proc > 1 if your system supports it and doesn't cause issues with NLTK/TensorFlow\n",
    "# Note: Multiprocessing with NLTK can sometimes be tricky. Start with num_proc=1 if issues arise.\n",
    "augmented_train_dataset = small_train_dataset.map(augment_fn, num_proc=1) # Start with 1 process\n",
    "print(\"Augmentation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57662428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of augmented training data:\n",
      "Original: My favourite food is anything I didn't have to cook myself.\n",
      "Augmented: My favourite food is anything I didn't have to cook myself.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Augmented: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Labels: [27]\n",
      "\n",
      "Original: WHY THE FUCK IS BAYLESS ISOING\n",
      "Augmented: WHY THE FUCK IS BAYLESS ISOING\n",
      "Labels: [2]\n",
      "\n",
      "Original: To make her feel threatened\n",
      "Augmented: To make her feel threatened\n",
      "Labels: [14]\n",
      "\n",
      "Original: Dirty Southern Wankers\n",
      "Augmented: Dirty Southern Wankers\n",
      "Labels: [3]\n",
      "\n",
      "Original: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Augmented: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Labels: [26]\n",
      "\n",
      "Original: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Augmented: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait 😝\n",
      "Labels: [15]\n",
      "\n",
      "Original: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Augmented: We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\n",
      "Labels: [8, 20]\n",
      "\n",
      "Original: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Augmented: Damn youtube and outrage drama is super lucrative for reddit\n",
      "Labels: [0]\n",
      "\n",
      "Original: It might be linked to the trust factor of your friend.\n",
      "Augmented: It might be linked to the trust factor of your friend.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Augmented: Demographics? I don’t know anybody under 35 who has cable tv.\n",
      "Labels: [6]\n",
      "\n",
      "Original: Aww... she'll probably come around eventually, I'm sure she was just jealous of [NAME]... I mean, what woman wouldn't be! lol \n",
      "Augmented: Aww... she 'll belike number around eventually, I 'm sure she was just envious of [ Gens ]... I imply, what woman wouldn't be! lol\n",
      "Labels: [1, 4]\n",
      "\n",
      "Original: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Augmented: Hello everyone. Im from Toronto as well. Can call and visit in personal if needed.\n",
      "Labels: [27]\n",
      "\n",
      "Original: R/sleeptrain Might be time for some sleep training. Take a look and try to feel out what's right for your family.\n",
      "Augmented: R/sleeptrain Power be meter for some sleep preparation. Take a look and try to feel out what's veracious for your house.\n",
      "Labels: [5]\n",
      "\n",
      "Original: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Augmented: [NAME] - same fucking problem, slightly better command of the English language.\n",
      "Labels: [3]\n",
      "\n",
      "Original: Shit, I guess I accidentally bought a Pay-Per-View boxing match\n",
      "Augmented: Shite, I judge I accidentally corrupt a Pay-Per-View boxing couple\n",
      "Labels: [3, 12]\n",
      "\n",
      "Original: Thank you friend\n",
      "Augmented: Thank you friend\n",
      "Labels: [15]\n",
      "\n",
      "Original: Fucking coward.\n",
      "Augmented: Fucking coward.\n",
      "Labels: [2]\n",
      "\n",
      "Original: that is what retardation looks like\n",
      "Augmented: that is what retardation looks like\n",
      "Labels: [27]\n",
      "\n",
      "Original: Maybe that’s what happened to the great white at Houston zoo\n",
      "Augmented: Maybe that’s what happened to the great white at Houston zoo\n",
      "Labels: [6, 22]\n",
      "\n",
      "Original: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Augmented: I never thought it was at the same moment, but sometimes after [NAME] sacrifice... sounds logical\n",
      "Labels: [6, 9, 27]\n",
      "\n",
      "Original: i got a bump and a bald spot. i feel dumb <3\n",
      "Augmented: i got a bump and a bald patch. i finger dull < 3\n",
      "Labels: [12]\n",
      "\n",
      "Original: You are going to do the dishes now\n",
      "Augmented: You are going to do the dishes now\n",
      "Labels: [27]\n",
      "\n",
      "Original: Slowing things down now\n",
      "Augmented: Slowing things down now\n",
      "Labels: [27]\n",
      "\n",
      "Original: His name has already been released. Just can't post it here.\n",
      "Augmented: His name has already been released. Just can't post it here.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Stupidly stubborn / stubbornly stupid\n",
      "Augmented: Stupidly stubborn / stubbornly stupid\n",
      "Labels: [2]\n",
      "\n",
      "Original: Mine was apparently [NAME] and the giant peach!\n",
      "Augmented: Mine was apparently [NAME] and the giant peach!\n",
      "Labels: [27]\n",
      "\n",
      "Original: I miss them being alive\n",
      "Augmented: I miss them being live\n",
      "Labels: [16, 25]\n",
      "\n",
      "Original: Super, thanks\n",
      "Augmented: Super, thanks\n",
      "Labels: [15]\n",
      "\n",
      "Original: A new study just came out from China that it's actually too late.\n",
      "Augmented: A new bailiwick just derive out from China that it's really too late.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Augmented: Troll, bro. They know they're saying stupid shit. The motherfucker does nothing but stink up libertarian subs talking shit\n",
      "Labels: [2]\n",
      "\n",
      "Original: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Augmented: All sounds possible except the key, I can't see how it was missed in the first search. \n",
      "Labels: [6]\n",
      "\n",
      "Original: Your aunt has some damn nerve, though!\n",
      "Augmented: Your aunt has some damn nerve, though!\n",
      "Labels: [27]\n",
      "\n",
      "Original: Ok, then what the actual fuck is your plan?\n",
      "Augmented: Ok, then what the actual fuck is your plan?\n",
      "Labels: [2, 7]\n",
      "\n",
      "Original: What does FPTP have to do with the referendum?\n",
      "Augmented: What does FPTP have to do with the referendum?\n",
      "Labels: [6]\n",
      "\n",
      "Original: Happy to be able to help.\n",
      "Augmented: Happy to be able to help.\n",
      "Labels: [17]\n",
      "\n",
      "Original: 18 is hot but very bland, it's just here this blonde lady who is not as hot as blonde launch.\n",
      "Augmented: 18 is hot but very bland, it's just here this blond noblewoman who is not as hot as blond launching.\n",
      "Labels: [27]\n",
      "\n",
      "Original: Famous for his 3-4 Defense\n",
      "Augmented: Famous for his 3-4 Refutation\n",
      "Labels: [0]\n",
      "\n",
      "Original: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Augmented: Pretty sure I’ve seen this. He swings away with the harness he is wearing. Still looks painful but I think he lives\n",
      "Labels: [25]\n",
      "\n",
      "Original: When I feel down I listen to music.\n",
      "Augmented: When I feel down I listen to music.\n",
      "Labels: [27]\n",
      "\n",
      "Original: aw, thanks! I appreciate that! \n",
      "Augmented: aw, thanks! I appreciate that! \n",
      "Labels: [0, 15]\n",
      "\n",
      "Original: Thanks! I love watching him every week\n",
      "Augmented: Thanks! I love watching him every week\n",
      "Labels: [15, 18]\n",
      "\n",
      "Original: I read on a different post that he died shortly after of internal injuries.\n",
      "Augmented: I read on a different post that he died shortly after of internal injuries.\n",
      "Labels: [16, 27]\n",
      "\n",
      "Original: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Augmented: Honestly it sounds exhausting being married to him. Maybe it will be better for you in the long run.\n",
      "Labels: [27]\n",
      "\n",
      "Original: It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\n",
      "Augmented: It's screwball how far Photoshop has fare. Underwater bridgework?!! NEVER!!!\n",
      "Labels: [7, 13]\n",
      "\n",
      "Original: I wouldn't let a sweet potato dictate decisions, ever.\n",
      "Augmented: I wouldn't let a fresh spud dictate conclusion, ever.\n",
      "Labels: [10]\n",
      "\n",
      "Original: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Augmented: It's true though. He either gets no shirt and freezes to death or wears a stupid looking butchers cape. I hope he gets something better next season\n",
      "Labels: [20]\n",
      "\n",
      "Original: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Augmented: It's a better option because it's my life and none of your business? Lmfao, who are you\n",
      "Labels: [27]\n",
      "\n",
      "Original: Sack, shaft, and tip. The trifecta. \n",
      "Augmented: Sack, shaft, and tip. The trifecta. \n",
      "Labels: [27]\n",
      "\n",
      "Original: I know. My question was if they **used** to compete in T5-TTT2.\n",
      "Augmented: I live. My interrogation was if they * * used * * to vie in T5-TTT2.\n",
      "Labels: [27]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print augmented data\n",
    "print(\"Sample of augmented training data:\")\n",
    "\n",
    "for i in range(50):\n",
    "    print(f\"Original: {small_train_dataset[i]['text']}\")\n",
    "    print(f\"Augmented: {augmented_train_dataset[i]['text']}\")\n",
    "    print(f\"Labels: {augmented_train_dataset[i]['labels']}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d906f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 6979.04 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 38330.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Tokenize the small dataset\n",
    "small_emotions_encoded = {}\n",
    "small_emotions_encoded['train'] = small_emotion_dataset['train'].map(tokenize, batched=True, batch_size=None)\n",
    "small_emotions_encoded['test'] = small_emotion_dataset['test'].map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "print(small_emotions_encoded['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f6d48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 24505.29 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 24083.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing columns: ['labels']\n",
      "Renaming 'multi_hot_labels' to 'labels'\n",
      "Calculating sample weights...\n",
      "Sample weights calculated.\n",
      "Setting dataset format to TensorFlow\n",
      "Creating tf.data.Dataset objects with sample weights...\n",
      "Datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "NUM_CLASSES = 28  # Define the number of classes (matches emotions_id2label)\n",
    "\n",
    "def create_multi_hot_labels(example):\n",
    "    \"\"\"Convert the list of labels into a multi-hot encoded vector.\"\"\"\n",
    "    multi_hot_label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "    if 'labels' in example and isinstance(example['labels'], list) and len(example['labels']) > 0:\n",
    "        for label_id in example['labels']:\n",
    "            if isinstance(label_id, int) and 0 <= label_id < NUM_CLASSES:\n",
    "                multi_hot_label[label_id] = 1.0\n",
    "    example['multi_hot_labels'] = multi_hot_label\n",
    "    return example\n",
    "\n",
    "# Apply the conversion to add the new multi-hot label column\n",
    "small_emotions_encoded['train'] = small_emotions_encoded['train'].map(create_multi_hot_labels)\n",
    "small_emotions_encoded['test'] = small_emotions_encoded['test'].map(create_multi_hot_labels)\n",
    "\n",
    "# Remove the original 'labels' column and any leftover 'label_int'\n",
    "columns_to_remove = [col for col in ['label_int', 'labels'] if col in small_emotions_encoded['train'].features]\n",
    "if columns_to_remove:\n",
    "    print(f\"Removing existing columns: {columns_to_remove}\")\n",
    "    small_emotions_encoded['train'] = small_emotions_encoded['train'].remove_columns(columns_to_remove)\n",
    "    small_emotions_encoded['test'] = small_emotions_encoded['test'].remove_columns(columns_to_remove)\n",
    "\n",
    "# Rename the new column 'multi_hot_labels' to 'labels'\n",
    "if 'multi_hot_labels' in small_emotions_encoded['train'].features:\n",
    "    print(\"Renaming 'multi_hot_labels' to 'labels'\")\n",
    "    small_emotions_encoded['train'] = small_emotions_encoded['train'].rename_column('multi_hot_labels', 'labels')\n",
    "if 'multi_hot_labels' in small_emotions_encoded['test'].features:\n",
    "    small_emotions_encoded['test'] = small_emotions_encoded['test'].rename_column('multi_hot_labels', 'labels')\n",
    "\n",
    "# --- Add Sample Weight Calculation ---\n",
    "print(\"Calculating sample weights...\")\n",
    "# Get all multi-hot labels from the training set as a NumPy array\n",
    "train_labels_np = np.array(small_emotions_encoded['train']['labels'])\n",
    "# Count frequency of each label (column-wise sum)\n",
    "label_counts = np.sum(train_labels_np, axis=0)\n",
    "total_samples = len(train_labels_np)\n",
    "\n",
    "# Calculate weight for each class (inverse frequency, smoothed)\n",
    "class_weights_calc = {}\n",
    "for i in range(NUM_CLASSES):\n",
    "    # Avoid division by zero for labels that might not appear in the subset\n",
    "    count = label_counts[i] if label_counts[i] > 0 else 1\n",
    "    class_weights_calc[i] = total_samples / (NUM_CLASSES * count)\n",
    "\n",
    "# Calculate weight for each sample: max weight of its positive labels\n",
    "sample_weights_np = np.zeros(total_samples, dtype=np.float32)\n",
    "for i in range(total_samples):\n",
    "    sample_label_indices = np.where(train_labels_np[i] == 1.0)[0]\n",
    "    if len(sample_label_indices) > 0:\n",
    "        sample_weights_np[i] = max(class_weights_calc[idx] for idx in sample_label_indices)\n",
    "    else:\n",
    "        # Assign a default weight (e.g., 1.0 or average) for samples with no positive labels\n",
    "        sample_weights_np[i] = 1.0\n",
    "print(\"Sample weights calculated.\")\n",
    "# --- End Sample Weight Calculation ---\n",
    "\n",
    "\n",
    "# Set format to tensorflow\n",
    "feature_cols = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "label_col = \"labels\"\n",
    "cols_to_set_format = feature_cols + [label_col]\n",
    "\n",
    "actual_train_cols = list(small_emotions_encoded['train'].features)\n",
    "actual_test_cols = list(small_emotions_encoded['test'].features)\n",
    "final_train_cols = [col for col in cols_to_set_format if col in actual_train_cols]\n",
    "final_test_cols = [col for col in cols_to_set_format if col in actual_test_cols]\n",
    "\n",
    "if all(col in final_train_cols for col in cols_to_set_format) and \\\n",
    "   all(col in final_test_cols for col in cols_to_set_format):\n",
    "    print(\"Setting dataset format to TensorFlow\")\n",
    "    # Don't set format yet, extract numpy arrays first, then create dataset\n",
    "else:\n",
    "     raise ValueError(f\"Error: Could not find all necessary columns. Train has: {actual_train_cols}, Test has: {actual_test_cols}. Needed: {cols_to_set_format}\")\n",
    "\n",
    "\n",
    "# Extract features and labels as numpy arrays before creating dataset\n",
    "train_features_np = {col: np.array(small_emotions_encoded['train'][col]) for col in feature_cols}\n",
    "train_labels_np = np.array(small_emotions_encoded['train']['labels']) # Already have this from weight calc\n",
    "\n",
    "test_features_np = {col: np.array(small_emotions_encoded['test'][col]) for col in feature_cols}\n",
    "test_labels_np = np.array(small_emotions_encoded['test']['labels'])\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "BATCH_SIZE = 32 # Keep batch size reasonable\n",
    "\n",
    "print(\"Creating tf.data.Dataset objects with sample weights...\")\n",
    "# Modify train_dataset to yield (features, labels, sample_weights)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_features_np, train_labels_np, sample_weights_np)\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(len(sample_weights_np)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) # Add prefetch\n",
    "\n",
    "# Test dataset remains (features, labels)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_features_np, test_labels_np)\n",
    ")\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) # Add prefetch\n",
    "print(\"Datasets created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e067bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        # Change activation to 'sigmoid' for multi-label classification\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Make sure we handle the case when inputs is a dictionary\n",
    "        outputs = self.bert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            token_type_ids=inputs['token_type_ids'],\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce5b9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NUM_CLASSES if not defined globally earlier\n",
    "try:\n",
    "    NUM_CLASSES\n",
    "except NameError:\n",
    "    NUM_CLASSES = 28 # Set default if run out of order\n",
    "\n",
    "# Create a shared emotion prediction function for multi-label output\n",
    "def predict_emotion(text, model, threshold=0.5):\n",
    "    \"\"\"Predict multiple emotions for a given text using the provided model and threshold\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    predictions = model(inputs) # Shape: (1, NUM_CLASSES)\n",
    "\n",
    "    # --- Add this line temporarily ---\n",
    "    # print(f\"Raw probabilities for '{text}': {predictions[0].numpy()}\")\n",
    "    # --- End of added line ---\n",
    "\n",
    "    predicted_labels_indices = tf.where(predictions[0] > threshold).numpy().flatten()\n",
    "    predicted_emotions = []\n",
    "    confidences = []\n",
    "    if len(predicted_labels_indices) > 0:\n",
    "        for index in predicted_labels_indices:\n",
    "            predicted_emotions.append(emotions_id2label[index])\n",
    "            confidences.append(float(predictions[0][index]))\n",
    "    else:\n",
    "        # Optional: If no label passes threshold, predict the highest one or 'neutral'\n",
    "        highest_prob_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "        predicted_emotions.append(emotions_id2label[highest_prob_index])\n",
    "        confidences.append(float(predictions[0][highest_prob_index]))\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': text,\n",
    "        'emotions': predicted_emotions,\n",
    "        'confidences': confidences\n",
    "    }\n",
    "\n",
    "# Define test texts to use for both untrained and trained models\n",
    "test_texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"This makes me really angry.\",\n",
    "    \"I'm feeling very sad and disappointed.\",\n",
    "    \"That's really interesting, tell me more.\",\n",
    "    \"I am both excited and nervous about the presentation.\", # Example with multiple emotions\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76f343",
   "metadata": {},
   "source": [
    "## Analyze Test Texts with Untrained Model\n",
    "\n",
    "Let's first create and test our model before training to establish a baseline. This will show how the model performs with random weights, which we can compare to the fine-tuned model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87469b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with UNTRAINED model (random weights - multi-label):\n",
      "-------------------------------------------------------------\n",
      "Raw probabilities for 'I'm so happy today!': [0.64240456 0.7607768  0.41271386 0.17730935 0.18901902 0.5629299\n",
      " 0.2589481  0.74051845 0.30491742 0.36972788 0.62420976 0.6385397\n",
      " 0.773479   0.5157682  0.4701345  0.2500274  0.7306917  0.6394449\n",
      " 0.36029348 0.8246309  0.55854404 0.5302149  0.76650345 0.37005234\n",
      " 0.3335643  0.5530919  0.6025893  0.42961365]\n",
      "Text: I'm so happy today!\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6424045562744141), ('amusement', 0.7607768177986145), ('anger', 0.4127138555049896), ('annoyance', 0.17730934917926788), ('approval', 0.18901902437210083), ('caring', 0.5629299283027649), ('confusion', 0.25894808769226074), ('curiosity', 0.7405184507369995), ('desire', 0.30491742491722107), ('disappointment', 0.36972787976264954), ('disapproval', 0.6242097616195679), ('disgust', 0.6385396718978882), ('embarrassment', 0.7734789848327637), ('excitement', 0.5157681703567505), ('fear', 0.4701344966888428), ('gratitude', 0.2500273883342743), ('grief', 0.73069167137146), ('joy', 0.639444887638092), ('love', 0.3602934777736664), ('nervousness', 0.8246309161186218), ('optimism', 0.5585440397262573), ('pride', 0.5302149057388306), ('realization', 0.7665034532546997), ('relief', 0.3700523376464844), ('remorse', 0.33356431126594543), ('sadness', 0.5530918836593628), ('surprise', 0.6025893092155457), ('neutral', 0.4296136498451233)]\n",
      "\n",
      "Raw probabilities for 'This makes me really angry.': [0.7105702  0.4495927  0.30021673 0.12030686 0.22675344 0.66601413\n",
      " 0.24661225 0.71624833 0.25794938 0.34615096 0.847026   0.515566\n",
      " 0.7549007  0.5098272  0.5870253  0.14359905 0.74130607 0.79984975\n",
      " 0.64645827 0.8397953  0.5237779  0.24103284 0.6996865  0.509712\n",
      " 0.19584928 0.55426836 0.55354303 0.2610282 ]\n",
      "Text: This makes me really angry.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.710570216178894), ('amusement', 0.4495927095413208), ('anger', 0.3002167344093323), ('annoyance', 0.12030685693025589), ('approval', 0.22675344347953796), ('caring', 0.6660141348838806), ('confusion', 0.24661225080490112), ('curiosity', 0.7162483334541321), ('desire', 0.2579493820667267), ('disappointment', 0.3461509644985199), ('disapproval', 0.847025990486145), ('disgust', 0.5155659914016724), ('embarrassment', 0.7549006938934326), ('excitement', 0.509827196598053), ('fear', 0.5870252847671509), ('gratitude', 0.14359904825687408), ('grief', 0.7413060665130615), ('joy', 0.7998497486114502), ('love', 0.6464582681655884), ('nervousness', 0.8397952914237976), ('optimism', 0.5237779021263123), ('pride', 0.24103283882141113), ('realization', 0.6996865272521973), ('relief', 0.5097119808197021), ('remorse', 0.19584928452968597), ('sadness', 0.5542683601379395), ('surprise', 0.5535430312156677), ('neutral', 0.2610282003879547)]\n",
      "\n",
      "Raw probabilities for 'I'm feeling very sad and disappointed.': [0.69302386 0.5497563  0.31268802 0.14394036 0.17792343 0.5694339\n",
      " 0.18059951 0.78020424 0.32472548 0.3470153  0.79235536 0.5568674\n",
      " 0.78668785 0.47127572 0.48803985 0.18490036 0.72877246 0.7233436\n",
      " 0.4803924  0.81477314 0.5063062  0.3732838  0.732281   0.41234818\n",
      " 0.27347484 0.63971823 0.6020157  0.38929844]\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6930238604545593), ('amusement', 0.5497562885284424), ('anger', 0.31268802285194397), ('annoyance', 0.14394035935401917), ('approval', 0.17792342603206635), ('caring', 0.5694339275360107), ('confusion', 0.18059951066970825), ('curiosity', 0.7802042365074158), ('desire', 0.324725478887558), ('disappointment', 0.34701529145240784), ('disapproval', 0.7923553586006165), ('disgust', 0.5568674206733704), ('embarrassment', 0.7866878509521484), ('excitement', 0.4712757170200348), ('fear', 0.48803985118865967), ('gratitude', 0.18490035831928253), ('grief', 0.7287724614143372), ('joy', 0.7233436107635498), ('love', 0.4803923964500427), ('nervousness', 0.8147731423377991), ('optimism', 0.5063061714172363), ('pride', 0.3732838034629822), ('realization', 0.7322810292243958), ('relief', 0.4123481810092926), ('remorse', 0.2734748423099518), ('sadness', 0.639718234539032), ('surprise', 0.6020156741142273), ('neutral', 0.3892984390258789)]\n",
      "\n",
      "Raw probabilities for 'That's really interesting, tell me more.': [0.6673931  0.68322885 0.39065474 0.17721848 0.20334406 0.5678425\n",
      " 0.24359901 0.68713826 0.2411537  0.4063151  0.74501145 0.6537101\n",
      " 0.742376   0.5087035  0.54594463 0.20996797 0.7202012  0.68980867\n",
      " 0.44109362 0.88491803 0.4915155  0.41533867 0.74841344 0.42702708\n",
      " 0.28181228 0.60828257 0.54636323 0.43637398]\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.6673930883407593), ('amusement', 0.6832288503646851), ('anger', 0.3906547427177429), ('annoyance', 0.1772184818983078), ('approval', 0.20334406197071075), ('caring', 0.5678424835205078), ('confusion', 0.24359901249408722), ('curiosity', 0.6871382594108582), ('desire', 0.24115370213985443), ('disappointment', 0.4063150882720947), ('disapproval', 0.7450114488601685), ('disgust', 0.653710126876831), ('embarrassment', 0.7423760294914246), ('excitement', 0.5087034702301025), ('fear', 0.5459446310997009), ('gratitude', 0.2099679708480835), ('grief', 0.7202011942863464), ('joy', 0.6898086667060852), ('love', 0.4410936236381531), ('nervousness', 0.8849180340766907), ('optimism', 0.49151548743247986), ('pride', 0.4153386652469635), ('realization', 0.7484134435653687), ('relief', 0.4270270764827728), ('remorse', 0.28181228041648865), ('sadness', 0.6082825660705566), ('surprise', 0.5463632345199585), ('neutral', 0.4363739788532257)]\n",
      "\n",
      "Raw probabilities for 'I am both excited and nervous about the presentation.': [0.70417804 0.5417714  0.35355198 0.12004349 0.15688144 0.60557324\n",
      " 0.20196846 0.79114014 0.37523136 0.3252459  0.78376293 0.5220251\n",
      " 0.7642598  0.4508476  0.45240316 0.20666341 0.76410455 0.76305735\n",
      " 0.5087837  0.7735311  0.5146118  0.41281584 0.7219109  0.36448473\n",
      " 0.26890585 0.6005785  0.60714704 0.33757466]\n",
      "Text: I am both excited and nervous about the presentation.\n",
      "Predicted emotions: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "Confidences: [('admiration', 0.7041780352592468), ('amusement', 0.541771411895752), ('anger', 0.353551983833313), ('annoyance', 0.12004348635673523), ('approval', 0.1568814367055893), ('caring', 0.6055732369422913), ('confusion', 0.2019684612751007), ('curiosity', 0.7911401391029358), ('desire', 0.3752313554286957), ('disappointment', 0.3252458870410919), ('disapproval', 0.7837629318237305), ('disgust', 0.5220251083374023), ('embarrassment', 0.7642598152160645), ('excitement', 0.4508475959300995), ('fear', 0.45240315794944763), ('gratitude', 0.20666341483592987), ('grief', 0.7641045451164246), ('joy', 0.7630573511123657), ('love', 0.5087836980819702), ('nervousness', 0.7735310792922974), ('optimism', 0.5146117806434631), ('pride', 0.4128158390522003), ('realization', 0.7219108939170837), ('relief', 0.3644847273826599), ('remorse', 0.2689058482646942), ('sadness', 0.6005784869194031), ('surprise', 0.6071470379829407), ('neutral', 0.33757466077804565)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model for baseline comparison\n",
    "# Ensure the base model 'model' is loaded correctly from cell 2\n",
    "untrained_classifier = BERTForClassification(model, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compile the model for multi-label classification\n",
    "untrained_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5),\n",
    "    # Use BinaryCrossentropy for multi-label with sigmoid activation\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    # Use BinaryAccuracy for multi-label evaluation\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "print(\"Predictions with UNTRAINED model (random weights - multi-label):\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Get predictions from untrained model using our updated shared function\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, untrained_classifier, threshold=0.1) # Lower threshold for untrained might show more random outputs\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotions: {result['emotions']}\")\n",
    "    # Zip confidences with emotions for clarity\n",
    "    emotion_confidence_pairs = list(zip(result['emotions'], result['confidences']))\n",
    "    print(f\"Confidences: {emotion_confidence_pairs}\")\n",
    "    # print(f\"Confidences: {[f'{c:.4f}' for c in result['confidences']]}\")\n",
    "    print()\n",
    "\n",
    "# Evaluating accuracy on the test set for an untrained multi-label model isn't very informative\n",
    "# untrained_loss, untrained_accuracy = untrained_classifier.evaluate(test_dataset, verbose=0)\n",
    "# print(f\"Untrained model test accuracy (BinaryAccuracy): {untrained_accuracy:.4f}\")\n",
    "# Random baseline for BinaryAccuracy depends on label distribution, harder to interpret than single-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d127d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model with AUC, Precision, Recall...\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Suggested Change for Cell ID: d127d7b4 ---\n",
    "\n",
    "# Update num_classes if not already defined\n",
    "try:\n",
    "    NUM_CLASSES\n",
    "except NameError:\n",
    "    NUM_CLASSES = 28\n",
    "\n",
    "# Define the model - ensure 'model' (the base BERT model) is loaded\n",
    "classifier = BERTForClassification(model, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compile the model for multi-label classification with more metrics\n",
    "print(\"Compiling model with AUC, Precision, Recall...\")\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5), # Consider trying AdamW later\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), # Correct loss for multi-label sigmoid\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(multi_label=True, name='auc'), # Good overall multi-label metric\n",
    "        tf.keras.metrics.Precision(name='precision'), # How many selected items are relevant?\n",
    "        tf.keras.metrics.Recall(name='recall') # How many relevant items are selected?\n",
    "        ]\n",
    ")\n",
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9df6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-label model training with sample weights...\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 720s 2s/step - loss: 0.1865 - accuracy: 0.9510 - auc: 0.5882 - precision: 0.0749 - recall: 0.0141 - val_loss: 0.1423 - val_accuracy: 0.9610 - val_auc: 0.7653 - val_precision: 0.9571 - val_recall: 0.0579\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 741s 2s/step - loss: 0.1367 - accuracy: 0.9604 - auc: 0.8260 - precision: 0.8304 - recall: 0.0787 - val_loss: 0.1210 - val_accuracy: 0.9631 - val_auc: 0.8775 - val_precision: 0.7541 - val_recall: 0.1590\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 713s 2s/step - loss: 0.1108 - accuracy: 0.9633 - auc: 0.8965 - precision: 0.7561 - recall: 0.1926 - val_loss: 0.1120 - val_accuracy: 0.9642 - val_auc: 0.8926 - val_precision: 0.7273 - val_recall: 0.2143\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 731s 2s/step - loss: 0.0905 - accuracy: 0.9652 - auc: 0.9333 - precision: 0.7311 - recall: 0.2792 - val_loss: 0.1090 - val_accuracy: 0.9641 - val_auc: 0.9009 - val_precision: 0.6396 - val_recall: 0.2990\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 734s 2s/step - loss: 0.0773 - accuracy: 0.9676 - auc: 0.9498 - precision: 0.7283 - recall: 0.3700 - val_loss: 0.1068 - val_accuracy: 0.9629 - val_auc: 0.9099 - val_precision: 0.5896 - val_recall: 0.3328\n",
      "Training finished.\n",
      "Evaluating model on test set...\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.1068 - accuracy: 0.9629 - auc: 0.9099 - precision: 0.5896 - recall: 0.3328\n",
      "\n",
      "Test Set Evaluation Results:\n",
      "- loss: 0.1068\n",
      "- accuracy: 0.9629\n",
      "- auc: 0.9099\n",
      "- precision: 0.5896\n",
      "- recall: 0.3328\n"
     ]
    }
   ],
   "source": [
    "# --- Suggested Change for Cell ID: e9df6074 ---\n",
    "\n",
    "# Train the model\n",
    "# Sample weights are now included in train_dataset, so no class_weight argument needed\n",
    "# Ensure train_dataset and test_dataset are correctly defined from the previous cell\n",
    "\n",
    "print(\"Starting multi-label model training with sample weights...\")\n",
    "# Consider adding callbacks like EarlyStopping or ModelCheckpoint for longer runs\n",
    "# callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=3, mode='max', restore_best_weights=True),\n",
    "#     tf.keras.callbacks.ModelCheckpoint('best_emotion_model.keras', save_best_only=True, monitor='val_auc', mode='max')\n",
    "# ]\n",
    "\n",
    "history = classifier.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,  # Adjust epochs as needed, more data might require more/fewer epochs\n",
    "    validation_data=test_dataset\n",
    "    # callbacks=callbacks # Uncomment to use callbacks\n",
    ")\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = classifier.evaluate(test_dataset, verbose=1) # Use verbose=1 to see progress\n",
    "\n",
    "# Print evaluation results dynamically based on compiled metrics\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for name, value in zip(classifier.metrics_names, results):\n",
    "    print(f\"- {name}: {value:.4f}\")\n",
    "\n",
    "# Example: Accessing specific metrics if needed\n",
    "# test_loss = results[classifier.metrics_names.index('loss')]\n",
    "# test_auc = results[classifier.metrics_names.index('auc')]\n",
    "# print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f61dbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with TRAINED multi-label model:\n",
      "----------------------------------------\n",
      "Text: I'm so happy today!\n",
      "Predicted emotions: ['excitement', 'joy']\n",
      "Confidences: [('excitement', 0.12582339346408844), ('joy', 0.7270192503929138)]\n",
      "\n",
      "Text: This makes me really angry.\n",
      "Predicted emotions: ['anger', 'annoyance']\n",
      "Confidences: [('anger', 0.702703595161438), ('annoyance', 0.11748744547367096)]\n",
      "\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotions: ['disappointment', 'grief', 'sadness']\n",
      "Confidences: [('disappointment', 0.3193146586418152), ('grief', 0.1527215540409088), ('sadness', 0.8931897878646851)]\n",
      "\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotions: ['excitement', 'joy']\n",
      "Confidences: [('excitement', 0.731715202331543), ('joy', 0.12100547552108765)]\n",
      "\n",
      "Text: I am both excited and nervous about the presentation.\n",
      "Predicted emotions: ['fear', 'nervousness']\n",
      "Confidences: [('fear', 0.31013643741607666), ('nervousness', 0.45450836420059204)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions with TRAINED multi-label model:\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Use the updated shared function with the trained model\n",
    "prediction_threshold = 0.1 # Adjust threshold as needed based on validation performance\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, classifier, threshold=prediction_threshold)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotions: {result['emotions']}\")\n",
    "    # Zip confidences with emotions for clarity\n",
    "    emotion_confidence_pairs = list(zip(result['emotions'], result['confidences']))\n",
    "    print(f\"Confidences: {emotion_confidence_pairs}\")\n",
    "    # print(f\"Confidences: {[f'{c:.4f}' for c in result['confidences']]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
