{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cb4c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bcfc88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca87985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "emotion_dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "print(emotion_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa5c1b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 43410\n",
      "Small train size: 1000\n",
      "Original test size: 5427\n",
      "Small test size: 200\n"
     ]
    }
   ],
   "source": [
    "# Select a larger subset of examples for better training\n",
    "# Using 1000 examples instead of 200 for better learning while still keeping training fast\n",
    "small_train_dataset = emotion_dataset['train'].select(range(1000))\n",
    "small_test_dataset = emotion_dataset['test'].select(range(200))\n",
    "\n",
    "# Create a new small dataset with the reduced splits\n",
    "small_emotion_dataset = {\n",
    "    'train': small_train_dataset,\n",
    "    'test': small_test_dataset\n",
    "}\n",
    "\n",
    "print(f\"Original train size: {len(emotion_dataset['train'])}\")\n",
    "print(f\"Small train size: {len(small_train_dataset)}\")\n",
    "print(f\"Original test size: {len(emotion_dataset['test'])}\")\n",
    "print(f\"Small test size: {len(small_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efe6e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of emotions dataset:\n",
      "{'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [27], 'id': 'eebbqej'}\n",
      "Number of labels: 28\n"
     ]
    }
   ],
   "source": [
    "emotions_id2label = {\n",
    "    0: 'admiration',\n",
    "    1: 'amusement',\n",
    "    2: 'anger',\n",
    "    3: 'annoyance',\n",
    "    4: 'approval',\n",
    "    5: 'caring',\n",
    "    6: 'confusion',\n",
    "    7: 'curiosity',\n",
    "    8: 'desire',\n",
    "    9: 'disappointment',\n",
    "    10: 'disapproval',\n",
    "    11: 'disgust',\n",
    "    12: 'embarrassment',\n",
    "    13: 'excitement',\n",
    "    14: 'fear',\n",
    "    15: 'gratitude',\n",
    "    16: 'grief',\n",
    "    17: 'joy',\n",
    "    18: 'love',\n",
    "    19: 'nervousness',\n",
    "    20: 'optimism',\n",
    "    21: 'pride',\n",
    "    22: 'realization',\n",
    "    23: 'relief',\n",
    "    24: 'remorse',\n",
    "    25: 'sadness',\n",
    "    26: 'surprise',\n",
    "    27: 'neutral'  # Last entry (no comma)\n",
    "}\n",
    "\n",
    "emotions_label2id = {v: k for k, v in emotions_id2label.items()}\n",
    "\n",
    "# Print dataset info to verify we understand what we're working with\n",
    "print(\"Sample of emotions dataset:\")\n",
    "print(small_emotion_dataset[\"train\"][0])\n",
    "print(f\"Number of labels: {len(emotions_id2label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d906f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Tokenize the small dataset\n",
    "small_emotions_encoded = {}\n",
    "small_emotions_encoded['train'] = small_emotion_dataset['train'].map(tokenize, batched=True, batch_size=None)\n",
    "small_emotions_encoded['test'] = small_emotion_dataset['test'].map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "print(small_emotions_encoded['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f6d48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 9104.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# We need to have numerical labels instead of lists, so let's select the first label for each example\n",
    "# Let's perform a preprocessing step to convert labels to simple integers first\n",
    "\n",
    "def convert_labels_to_int(example):\n",
    "    \"\"\"Convert the list of labels to a single integer (take the first one)\"\"\"\n",
    "    # If the labels list is not empty, take the first label\n",
    "    if example['labels'] and len(example['labels']) > 0:\n",
    "        example['label_int'] = int(example['labels'][0])  # Ensure it's an integer\n",
    "    else:\n",
    "        # Default to 'neutral' (27) if no labels\n",
    "        example['label_int'] = 27\n",
    "    return example\n",
    "\n",
    "# Apply the conversion to add a new integer label column\n",
    "small_emotions_encoded['train'] = small_emotions_encoded['train'].map(convert_labels_to_int)\n",
    "small_emotions_encoded['test'] = small_emotions_encoded['test'].map(convert_labels_to_int)\n",
    "\n",
    "# Create TensorFlow datasets manually for better control\n",
    "import numpy as np\n",
    "\n",
    "# For training data\n",
    "train_input_ids = np.array(small_emotions_encoded['train']['input_ids'])\n",
    "train_attention_mask = np.array(small_emotions_encoded['train']['attention_mask'])\n",
    "train_token_type_ids = np.array(small_emotions_encoded['train']['token_type_ids'])\n",
    "train_labels = np.array(small_emotions_encoded['train']['label_int'])\n",
    "\n",
    "# For test data\n",
    "test_input_ids = np.array(small_emotions_encoded['test']['input_ids'])\n",
    "test_attention_mask = np.array(small_emotions_encoded['test']['attention_mask'])\n",
    "test_token_type_ids = np.array(small_emotions_encoded['test']['token_type_ids'])\n",
    "test_labels = np.array(small_emotions_encoded['test']['label_int'])\n",
    "\n",
    "# setting BATCH_SIZE to a smaller value for the smaller dataset\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def order(inputs_dict, labels):\n",
    "    '''\n",
    "    This function will group all the inputs of BERT\n",
    "    into a single dictionary and then output it with\n",
    "    labels.\n",
    "    '''    \n",
    "    return inputs_dict, labels\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_features = {\n",
    "    'input_ids': train_input_ids,\n",
    "    'attention_mask': train_attention_mask,\n",
    "    'token_type_ids': train_token_type_ids\n",
    "}\n",
    "\n",
    "test_features = {\n",
    "    'input_ids': test_input_ids,\n",
    "    'attention_mask': test_attention_mask,\n",
    "    'token_type_ids': test_token_type_ids\n",
    "}\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f44ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 46), dtype=int64, numpy=\n",
      "array([[ 101, 1045, 2572, ...,    0,    0,    0],\n",
      "       [ 101, 2108, 2619, ...,    0,    0,    0],\n",
      "       [ 101, 3407, 9850, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1045, 2150, ...,    0,    0,    0],\n",
      "       [ 101, 2024, 2017, ...,    0,    0,    0],\n",
      "       [ 101, 2049, 7929, ...,    0,    0,    0]])>, 'attention_mask': <tf.Tensor: shape=(32, 46), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'token_type_ids': <tf.Tensor: shape=(32, 46), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>} \n",
      "\n",
      " tf.Tensor(\n",
      "[ 7  5  1 20  9  1  1  1 27 26  4  1 27 27  0  1  0  0 27 10  7  6 27  4\n",
      "  3 18 27 26 17 20  7  2], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "inp, out = next(iter(train_dataset)) # a batch from train_dataset\n",
    "print(inp, '\\n\\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e067bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Make sure we handle the case when inputs is a dictionary\n",
    "        outputs = self.bert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            token_type_ids=inputs['token_type_ids'],\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce5b9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shared emotion prediction function to use with both untrained and trained models\n",
    "def predict_emotion(text, model):\n",
    "    \"\"\"Predict emotion for a given text using the provided model\"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    \n",
    "    # Get prediction from model\n",
    "    prediction = model(inputs)\n",
    "    predicted_class = tf.argmax(prediction, axis=1).numpy()[0]\n",
    "    predicted_emotion = emotions_id2label[predicted_class]\n",
    "    \n",
    "    # Get confidence score\n",
    "    confidence = tf.nn.softmax(prediction, axis=1).numpy()[0][predicted_class]\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'emotion': predicted_emotion,\n",
    "        'confidence': float(confidence)\n",
    "    }\n",
    "\n",
    "# Define test texts to use for both untrained and trained models\n",
    "test_texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"This makes me really angry.\",\n",
    "    \"I'm feeling very sad and disappointed.\",\n",
    "    \"That's really interesting, tell me more.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76f343",
   "metadata": {},
   "source": [
    "## Analyze Test Texts with Untrained Model\n",
    "\n",
    "Let's first create and test our model before training to establish a baseline. This will show how the model performs with random weights, which we can compare to the fine-tuned model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87469b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with UNTRAINED model (random weights):\n",
      "---------------------------------------------\n",
      "Text: I'm so happy today!\n",
      "Predicted emotion: gratitude\n",
      "Confidence: 0.0399\n",
      "\n",
      "Text: This makes me really angry.\n",
      "Predicted emotion: surprise\n",
      "Confidence: 0.0395\n",
      "\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotion: surprise\n",
      "Confidence: 0.0393\n",
      "\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotion: surprise\n",
      "Confidence: 0.0393\n",
      "\n",
      "Untrained model test accuracy: 0.0300 (should be close to random guessing)\n",
      "Random baseline (1/28): 0.0357\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model for baseline comparison\n",
    "untrained_classifier = BERTForClassification(model, num_classes=28)\n",
    "\n",
    "# Compile the model with the same settings we'll use for training\n",
    "untrained_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Predictions with UNTRAINED model (random weights):\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Get predictions from untrained model using our shared function\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, untrained_classifier)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotion: {result['emotion']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Also evaluate on test dataset to get baseline accuracy\n",
    "untrained_loss, untrained_accuracy = untrained_classifier.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Untrained model test accuracy: {untrained_accuracy:.4f} (should be close to random guessing)\")\n",
    "print(f\"Random baseline (1/28): {1/28:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d127d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update num_classes to match the actual number of emotion classes (28)\n",
    "classifier = BERTForClassification(model, num_classes=28)\n",
    "\n",
    "# Add a smaller learning rate and a weight decay for better regularization\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9df6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights to handle imbalanced data:\n",
      "{0: 0.36075036075036077, 1: 0.6265664160401002, 2: 0.8710801393728222, 3: 0.6868131868131868, 4: 0.6053268765133172, 5: 2.100840336134454, 6: 1.1160714285714286, 7: 0.9157509157509157, 8: 3.5714285714285716, 9: 1.2755102040816326, 10: 0.8710801393728222, 11: 3.9682539682539684, 12: 7.142857142857143, 13: 3.5714285714285716, 14: 3.5714285714285716, 15: 0.8116883116883117, 16: 5.9523809523809526, 17: 1.4285714285714286, 18: 1.152073732718894, 19: 11.904761904761905, 20: 1.7857142857142858, 21: 35.714285714285715, 22: 3.9682539682539684, 23: 35.714285714285715, 24: 4.464285714285714, 25: 1.8796992481203008, 26: 1.6233766233766234, 27: 0.11825922421948912}\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 70s 2s/step - loss: 3.5324 - accuracy: 0.0330 - val_loss: 3.3964 - val_accuracy: 0.0250\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 3.3419 - accuracy: 0.0250 - val_loss: 3.3624 - val_accuracy: 0.0250\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 3.1397 - accuracy: 0.0540 - val_loss: 3.4348 - val_accuracy: 0.0300\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 2.8262 - accuracy: 0.1040 - val_loss: 3.2833 - val_accuracy: 0.0550\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 2.4143 - accuracy: 0.1680 - val_loss: 2.9705 - val_accuracy: 0.1800\n",
      "7/7 [==============================] - 4s 523ms/step - loss: 2.9705 - accuracy: 0.1800\n",
      "Test accuracy: 0.1800\n"
     ]
    }
   ],
   "source": [
    "# Add more epochs for better training (5 instead of 3)\n",
    "# Add class_weights to handle imbalanced data\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights to handle imbalanced data\n",
    "unique_classes = np.unique(train_labels)\n",
    "class_weights_array = compute_class_weight('balanced', classes=unique_classes, y=train_labels)\n",
    "class_weights = dict(zip(unique_classes, class_weights_array))\n",
    "\n",
    "print(\"Class weights to handle imbalanced data:\")\n",
    "print(class_weights)\n",
    "\n",
    "history = classifier.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,  # More epochs\n",
    "    validation_data=test_dataset,\n",
    "    class_weight=class_weights  # Add class weights\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = classifier.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f61dbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with TRAINED model:\n",
      "---------------------------\n",
      "Text: I'm so happy today!\n",
      "Predicted emotion: gratitude\n",
      "Confidence: 0.0435\n",
      "\n",
      "Text: This makes me really angry.\n",
      "Predicted emotion: remorse\n",
      "Confidence: 0.0394\n",
      "\n",
      "Text: I'm feeling very sad and disappointed.\n",
      "Predicted emotion: grief\n",
      "Confidence: 0.0496\n",
      "\n",
      "Text: That's really interesting, tell me more.\n",
      "Predicted emotion: curiosity\n",
      "Confidence: 0.0382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions with TRAINED model:\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Use the same shared function with our trained model\n",
    "for text in test_texts:\n",
    "    result = predict_emotion(text, classifier)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted emotion: {result['emotion']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8b014",
   "metadata": {},
   "source": [
    "## Multi-label vs Single-label Performance\n",
    "\n",
    "The multi-label approach has several advantages over single-label classification for emotion detection:\n",
    "\n",
    "1. **More accurate representation**: Texts often express multiple emotions simultaneously, which our model can now capture\n",
    "2. **More training signal**: By using all labels, the model learns from more data points per example\n",
    "3. **Better predictions**: We now output multiple emotion predictions with confidence scores, rather than forcing a single choice\n",
    "\n",
    "This is especially important for emotion detection, as emotions are often mixed (e.g., a text might express both surprise and joy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
