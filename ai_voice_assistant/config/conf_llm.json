{
    "default": {
        "llm_local": true,
        "model_name": "llama3.2",
        "creativity_presets": ["low", "medium", "high", "random"],
        "local": {
            "llm_path": "models/llama/llama-3.2-1b-instruct.Q4_K_M.gguf",
            "chat_format": "llama-3",
            "n_gpu_layers": -1,
            "repeat_penalty": 1.5,
            "n_ctx": 4096,
            "temperature": 1.5,
            "top_p": 0.95,
            "top_k": 60,
            "max_tokens": 2048
        }
    }
}